{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 02: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please upload your solution ipynb file to StudIP in your corresponding group folder before you have your `Testat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Decision Trees [2p]\n",
    "Draw/build the decision trees for the following boolean functions. Either use pen & paper or employ your ASCII artist within in the jupyter notebook.\n",
    "\n",
    "Note: $\\oplus := xor$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) $\\neg A \\cap B$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) $A \\oplus B$ "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) $A \\vee (B \\wedge C) \\vee (\\neg C \\wedge D)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) $(A \\rightarrow (B \\wedge \\neg C)) \\vee (A \\wedge B)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Entropy and Information Gain [5p]\n",
    "\n",
    "Attributes and their possible values:\n",
    "\n",
    "  * $raining = \\{yes, no\\}$\n",
    "  * $tired = \\{yes, no\\}$\n",
    "  * $late = \\{yes, no\\}$\n",
    "  * $distance = \\{short, medium, long\\}$\n",
    "\n",
    "Training examples:\n",
    "\n",
    "| #  | raining | tired | late | distance | attend_party |\n",
    "|----|---------|-------|------|----------|--------------|\n",
    "| 1  | yes | no  | no  | short  | **yes** |\n",
    "| 2  | yes | no  | yes | medium | **no**  |\n",
    "| 3  | no  | yes | no  | long   | **no**  |\n",
    "| 4  | yes | yes | yes | short  | **no**  |\n",
    "| 5  | yes | no  | no  | short  | **yes** |\n",
    "| 6  | no  | no  | no  | medium | **yes** |\n",
    "| 7  | no  | yes | no  | long   | **no**  |\n",
    "| 8  | yes | no  | yes | short  | **no**  |\n",
    "| 9  | yes | yes | no  | short  | **yes** |\n",
    "| 10 | no  | yes | no  | medium | **no**  |\n",
    "| 11 | no  | yes | no  | long   | **no**  |\n",
    "| 12 | no  | yes | yes | short  | **no**  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) [3p]\n",
    "Build the root node of a decision tree from the training examples given in the table above by calculate the information gain on all four attributes (raining, tired, late, distance).\n",
    "\n",
    "$$Gain(S,A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|}Entropy(S_v)$$\n",
    "$$Entropy(S) = -p_{\\oplus} log_{2} p_{\\oplus} - p_{\\ominus} log_{2} p_{\\ominus}$$\n",
    "\n",
    "\n",
    "\n",
    "$S_v$ is the subset of $S$ for which attribute A has value v. Example for attribute *tired*:\n",
    "$S_{yes} \\leftarrow [1+, 6−], |S_{yes}| = 7 \\quad \\quad \\quad S_{no} \\leftarrow [3+, 2−], |S_{no}| = 5$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) [2p]\n",
    "Perform the same calculation as in **a)** but use the gain ratio instead of the information gain. Does the result for the root node change?\n",
    "\n",
    "$$GainRatio(S,A) = \\frac{Gain(S,A)}{SplitInformation(S,A)}$$\n",
    "$$SplitInformation(S,A) = - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} log_{2} \\frac{S_{v}}{S}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Let’s assume the root node is a node which checks the value of the attribute has *distance*. Calculate the next level of the decision tree using the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "Implement the following two functions in Python. Take a look at the `assert`s to see how the function should behave. An assert is a condition that your function is required to pass. Most of the conditions here are taken from the lecture slides (ML-03, Slide 12 & 13). Don't worry if you do not get all asserts to pass, just comment them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) \n",
    "$$Entropy(S) = - \\sum_{i=1...c} p_i log_2 p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "def entropy(S):\n",
    "    \"\"\"\n",
    "    Calculate the entropy for a given target value set. \n",
    "    S: List of target classes for specific observations.\n",
    "    \"\"\"\n",
    "    p_i = [len([value for value in S if value == c])/len(S) for c in set(S)]\n",
    "    return - sum(p * log(p, 2) for p in p_i) \n",
    "\n",
    "\n",
    "# See ML-03, Slide 12 & 13\n",
    "assert entropy([1,1,1,0,0,0]) == 1.0\n",
    "assert round(entropy([1,1,1,1,0,0,0]), 3) == 0.985\n",
    "assert round(entropy([1,1,1,1,1,1,0]), 3) == 0.592\n",
    "assert round(entropy([1,1,1,1,1,1,0,0]), 3) == 0.811\n",
    "assert round(entropy([2,2,1,1,0,0]), 3) == 1.585\n",
    "assert round(entropy([2,2,2,1,0]), 3) == 1.371\n",
    "assert round(entropy([2,2,2,0,0]), 3) == 0.971"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) \n",
    "$$Gain(S,A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gain(S, A):\n",
    "    \"\"\"\n",
    "    Calculates the expected reduction in entropy due to sorting on A.\n",
    "    S: Target classes for observations in A.\n",
    "    A: Observations.\n",
    "    \"\"\"\n",
    "    sigma = 0\n",
    "    for v in set(A): # sets only contain unique values\n",
    "        S_v = [S[key] for (key, v_) in enumerate(A) if v_ == v]\n",
    "        sigma = sigma + ((len(S_v) / len(S)) * entropy(S_v))\n",
    "    return entropy(S) - sigma\n",
    "\n",
    "\n",
    "# See ML-03, Slide 12 & 13\n",
    "assert_S_ = [0,0,1,1,1,0,1,0,1,1,1,1,1,0]\n",
    "assert round(gain(assert_S_, [1,1,1,1,0,0,0,1,0,0,0,1,0,1]), 3) == .152\n",
    "assert round(gain(assert_S_, [0,1,0,0,0,1,1,0,0,0,1,1,0,1]), 3) == .048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just for future reference.\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "assert round(gain(iris.target, [r[2] for r in iris.data]), 3) == 1.446\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
