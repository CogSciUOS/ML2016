{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 05, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Perceptron Theory (x Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Perceptron (8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement a multilayer perceptron as described in the lecture. We start with the basic building block: the perceptron [ML-07 Slide 31]. As with a previous exercise it is possible to not use our premade code blocks but write the single Perceptron completly from scratch (the section for this can be found on the bottom of the exercise). You are free to use lambdas for this exercise. Those are handy constructs that allow to create small anonymous functions. For example if you want to write a little lambda function for addition it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addition = lambda x, y : x + y\n",
    "print(addition(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel uncomfortable with this use regular functions, but lambdas can shorten things up - so at least give it a try. The $TODO$'s in the following code segments guide you through what has to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "# TODO: Write the activation function (called actFun) and the output function (called outFun).\n",
    "\n",
    "# Activation function.\n",
    "actFun = lambda d, w: d @ np.transpose(w)\n",
    "\n",
    "# The output function determines the output of the neuron (1 if >0, else -1).\n",
    "outFun = lambda x: x > 0\n",
    "\n",
    "# TODO: Write a function that generates weights.\n",
    "def generateWeights(dims):\n",
    "    '''\n",
    "    Generates weights with the given bias for a number\n",
    "    of dimensions.\n",
    "    '''\n",
    "    W = rnd.rand(1, dims + 1)\n",
    "    W.shape = (1, dims + 1)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Testing the perceptron with a concrete example ##\n",
    "####################################################\n",
    "\n",
    "# Dimensions for our test.\n",
    "dims = 12 \n",
    "\n",
    "# Input is a row vector.\n",
    "D = np.append(1, rnd.rand(1, dims))\n",
    "D.shape = (1, dims + 1)\n",
    "\n",
    "#weights are stored in a vector\n",
    "W = generateWeights(dims)\n",
    "\n",
    "out = outFun(actFun(D, W))\n",
    "assert out == 1 or out == -1, \"The output has to be either 1 or -1, but was %d\" %out\n",
    "assert actFun(D, W).shape == (1, 1), \"The activation functions output should be one value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following $eval\\_network(t, D, W)$ function is used to measure the performance of your perceptron for the upcoming task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_network(t, D, W):\n",
    "    '''\n",
    "    This function takes the trained weights of a perceptron\n",
    "    and the input data (D) as well as the correct target values (t)\n",
    "    and computes the overall error rate of the perceptron.\n",
    "    '''\n",
    "    error = 0.0\n",
    "    size = np.max(D.shape)\n",
    "    for i in range(0, size):\n",
    "        out = outFun(actFun(D[i], W))\n",
    "        error = error + np.abs(t[i] - out)\n",
    "    # Normalize the error\n",
    "    return error/size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the above defined functions to train the perceptron to one of the following logical functions: AND, OR, NAND or NOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Now we train our perceptron! [ML-07, sl.33]\n",
    "#############################################\n",
    "\n",
    "# TODO write the update function for the weights dependent\n",
    "#      on epsilon, the target, the output and the input vector\n",
    "delta_fun = lambda eps,t,y,x: eps * (t-y) * x\n",
    "\n",
    "\n",
    "# TODO define suitable parameters for your problem\n",
    "eps = 0.1\n",
    "dims = 2\n",
    "training_size = 1250\n",
    "\n",
    "# TODO generate the weights\n",
    "W = generateWeights(dims)\n",
    "\n",
    "# Input\n",
    "# generate a list of truthvalue pairs\n",
    "D_i = rnd.rand(training_size, dims) > 0.5\n",
    "\n",
    "# pad the input with ones for the threshold/bias/w_0\n",
    "D = np.ones((training_size, dims+1))\n",
    "D[:,1:] = D_i \n",
    "D = np.matrix(D)\n",
    "\n",
    "# Example learn one of the logical functions: AND, OR, NAND, NOR\n",
    "op = lambda x1, x2: x1 and x2 #TODO change for other functions\n",
    "\n",
    "log_op = lambda row: op(row[0], row[1])\n",
    "labels = np.apply_along_axis(log_op, 1, D[:,1:])\n",
    "\n",
    "epochs = 20\n",
    "samp_size = 5\n",
    "\n",
    "for i in range(0,epochs):\n",
    "    #sample random from the training data\n",
    "    for idx in rnd.choice(range(0, training_size), samp_size):\n",
    "        y = outFun(actFun(D[idx], W))\n",
    "        W = W + delta_fun(eps, labels[idx], y, D[idx])\n",
    "\n",
    "\n",
    "#Print the overall performance of the Perceptron\n",
    "print(\"Overall error of the Perceptron: {:.2%}\".format(eval_network(labels, D, W)[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Space for complete own implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Activation Function and Backpropagation (x Points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
