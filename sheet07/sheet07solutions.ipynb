{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 05, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Perceptron Theory (x Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Multilayer Perceptron (8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Starting with a single Perceptron\n",
    "In this exercise you will implement a multilayer perceptron as described in the lecture. We start with the basic building block: the perceptron [ML-07 Slide 31]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "# TODO: Write the activation and output function.\n",
    "\n",
    "# Activation function.\n",
    "actFun = lambda d, w: d @ np.transpose(w)\n",
    "\n",
    "# The output function is determining the output of the neuron (1 if >0, else -1).\n",
    "outFun = lambda x: x > 0\n",
    "\n",
    "# TODO: Write a function that generates weights.\n",
    "\n",
    "def generateWeights(bias, dims):\n",
    "    '''\n",
    "    Generates weights with the given bias for a number\n",
    "    of dimensions.\n",
    "    '''\n",
    "    W = np.append(bias, rnd.rand(1, dims))\n",
    "    W.shape = (1, dims + 1)\n",
    "    return W\n",
    "\n",
    "####################################################\n",
    "## Testing the perceptron with a concrete example ##\n",
    "####################################################\n",
    "\n",
    "# Dimension of the data.\n",
    "dims = 12 \n",
    "\n",
    "# Input is a row vector.\n",
    "D = np.append(1, rnd.rand(1, dims))\n",
    "D.shape = (1, dims + 1)\n",
    "\n",
    "#weights are stored in a vector\n",
    "W = generateWeights(-1, dims)\n",
    "\n",
    "out = outFun(actFun(D, W))\n",
    "assert out == 1 or out == -1, \"The output has to be either 1 or -1, but was %d\" %out\n",
    "assert actFun(D, W).shape == (1, 1), \"The activation functions output should be one value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the above defined functions to train the perceptron to one of the following logical functions: AND, OR, NAND or NOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_network(t, D, W):\n",
    "    '''\n",
    "    This function takes the trained weights of a perceptron\n",
    "    and the input data (D) as well as the correct target values (t)\n",
    "    and computes the overall error rate of the perceptron.\n",
    "    '''\n",
    "    error = 0.0\n",
    "    size = np.max(D.shape)\n",
    "    for i in range(0, size):\n",
    "        out = outFun(actFun(D[i], W))\n",
    "        error = error + np.abs(t[i] - out)\n",
    "    # Normalize the error\n",
    "    return error/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall error of the Perceptron: 8.56%\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Now we train our perceptron! [ML-07, sl.33]\n",
    "#############################################\n",
    "\n",
    "# TODO write the update function for the weights dependent\n",
    "#      on epsilon, the target, the output and the input vector\n",
    "delta_fun = lambda eps,t,y,x: eps*(t-y)*x\n",
    "\n",
    "\n",
    "# Example learn one of the logical functions: AND, OR, NAND, NOR\n",
    "# TODO define suitable parameters for your problem\n",
    "\n",
    "eps = 0.1\n",
    "dims = 2\n",
    "training_size = 1250\n",
    "\n",
    "W = generateWeights(-0.2, dims)\n",
    "\n",
    "# Input\n",
    "D_i = rnd.rand(training_size, dims)\n",
    "D = np.ones((training_size, dims+1)) \n",
    "D[:,1:] = D_i #pad the input with ones for the bias\n",
    "D = np.matrix(D)\n",
    "\n",
    "# target function\n",
    "op = lambda x1, x2: x1 and x2 #TODO change for other functions\n",
    "log_op = lambda row: op(np.round(row[0]), np.round(row[1]))\n",
    "labels = np.apply_along_axis(log_op,1,D[:,1:])\n",
    "\n",
    "epochs = 1000\n",
    "samp_size = 40\n",
    "\n",
    "for i in range(0,epochs):\n",
    "    error = 0\n",
    "    #sample random from the training data\n",
    "    for idx in rnd.choice(range(0, training_size), samp_size):\n",
    "        y = outFun(actFun(D[idx], W))\n",
    "        W = W + delta_fun(eps, labels[idx], y, D[idx])\n",
    "        error = error + abs(labels[idx] - y)\n",
    "\n",
    "\n",
    "#Print the overall performance of the Perceptron\n",
    "print(\"Overall error of the Perceptron: {:.2%}\".format(eval_network(labels, D, W)[0,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, dimensions, eps=0.1):\n",
    "        self.w = np.random.rand(1 + dimensions)\n",
    "        self.eps = eps\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        s = self.w @ np.append(1, x)\n",
    "        return s > 0\n",
    "\n",
    "    def adapt(self, x, y, t):\n",
    "        self.w[np.newaxis] += self.eps * (t - y) * np.append(1, x)\n",
    "\n",
    "    def train(self, x, t):\n",
    "        y = self.evaluate(x)\n",
    "        self.adapt(x, y, t)\n",
    "        return y\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Perceptron({})\".format(len(self.w) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "X = np.matrix(list(itertools.product([0, 1], repeat=2)))\n",
    "\n",
    "AND = lambda X : np.all(X, 1)\n",
    "OR = lambda X : np.any(X, 1)\n",
    "NAND = lambda X : 1 - AND(X)\n",
    "NOR = lambda X : 1 - OR(X)\n",
    "XOR = lambda X : X[:,0] != X[:,1]\n",
    "NXOR = lambda X : 1 - XOR(X)\n",
    "\n",
    "T = AND(X)\n",
    "D = np.hstack((X, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perceptron has an error of 0.00%\n"
     ]
    }
   ],
   "source": [
    "p = Perceptron(2)\n",
    " \n",
    "for s in np.random.choice(range(4), 100):\n",
    "    x = D[s,0:2]\n",
    "    t = D[s,2]\n",
    "    y = p.train(x, t)\n",
    "\n",
    "error = 0\n",
    "for d in D:\n",
    "    x = d[0,0:2]\n",
    "    t = d[0,2]\n",
    "    y = p.evaluate(x)\n",
    "    error += np.abs(t - y) / len(D)\n",
    "\n",
    "print(\"The perceptron has an error of {:.2%}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "[ 0.28090908  0.14436183]\n",
      "[ 0.34964381  0.52136844]\n",
      "[ 0.33178455  0.37598814  0.05941852]\n",
      "[ 0.7288982   0.55305168]\n",
      "\n",
      "The multilayer perceptron has an error of 75.00%\n"
     ]
    }
   ],
   "source": [
    "input_layer = [Perceptron(1), Perceptron(1)]\n",
    "hidden_layer = [Perceptron(2)]\n",
    "output_layer = [Perceptron(1)]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_in, n_out, *n_hidden, eps=0.1):\n",
    "        self.eps = eps\n",
    "        self.layers = [[Perceptron(1) for _i in range(n_in)]]\n",
    "        for layer in n_hidden:\n",
    "            self.layers.append([Perceptron(len(self.layers[-1])) for _i in range(layer)])\n",
    "        self.layers.append([Perceptron(len(self.layers[-1])) for _i in range(n_out)])\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        self.o = [[p.evaluate(x[:,i]) for i, p in enumerate(self.layers[0])]]\n",
    "        for i, layer in enumerate(self.layers[1:]):\n",
    "            self.o.append([p.evaluate(self.o[i]) for p in layer])\n",
    "        return self.o[-1]\n",
    "\n",
    "    def adapt(self, t):\n",
    "        # calculate error signals delta\n",
    "        \n",
    "        # output layer\n",
    "        for i in range(len(self.layers[-1])):\n",
    "            o = np.matrix(self.o[-1][i])\n",
    "            self.layers[-1][i].delta = o * (1 - o) * (t - o)\n",
    "\n",
    "        # other layers, from back to front\n",
    "        for k in range(len(self.layers) - 2, -1, -1):\n",
    "            for i in range(len(self.layers[k])):\n",
    "                o = np.matrix(self.o[k][i])\n",
    "                delta = o * (1 - o) * np.sum([n.w[i] * n.delta for n in self.layers[k+1]])\n",
    "                self.layers[k][i].delta = delta\n",
    "        \n",
    "        # adapt weights\n",
    "        for k in range(len(self.layers) - 1):\n",
    "            for i in range(len(self.layers[k+1])):\n",
    "                self.layers[k+1][i].w[np.newaxis] += self.eps * self.layers[k+1][i].delta * np.matrix(self.o[k][i])\n",
    "\n",
    "    def train(self, x, t):\n",
    "        y = self.evaluate(x)\n",
    "        self.adapt(t)\n",
    "        for l in self.layers:\n",
    "            for p in l:\n",
    "                print(p.w)\n",
    "        print()\n",
    "\n",
    "mlp = MLP(2,1,1)\n",
    "\n",
    "for s in np.random.choice(range(4), 10):\n",
    "    x = D[s,0:2]\n",
    "    t = D[s,2]\n",
    "    y = mlp.train(x, t)\n",
    "\n",
    "error = 0\n",
    "for d in D:\n",
    "    x = d[0,0:2]\n",
    "    t = d[0,2]\n",
    "    y = mlp.evaluate(x)\n",
    "    error += np.abs(t - y) / len(D)\n",
    "\n",
    "print(\"The multilayer perceptron has an error of {[0]:.2%}\".format(error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Building up ot the Multilayer Perceptron\n",
    "In this exercise you will extend the model from above to a Multilayer Perceptron with several neurons aranged in several layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Activation Function and Backpropagation (x Points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
