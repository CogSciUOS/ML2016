{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 05, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Perceptron Theory (x Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Perceptron (8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement a multilayer perceptron as described in the lecture. We start with the basic building block: the perceptron [ML-07 Slide 31]. As with a previous exercise it is possible to not use our premade code blocks but write the single Perceptron completly from scratch (the section for this can be found on the bottom of the exercise). You are free to use lambdas for this exercise. Those are handy constructs that allow to create small anonymous functions. For example if you want to write a little lambda function for addition it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addition = lambda x, y : x + y\n",
    "print(addition(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel uncomfortable with this use regular functions, but lambdas can shorten things up - so at least give it a try. The $TODO$'s in the following code segments guide you through what has to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "# TODO: Write the activation function (called actFun) and the output function (called outFun).\n",
    "\n",
    "# Activation function.\n",
    "actFun = lambda d, w: d @ np.transpose(w)\n",
    "\n",
    "# The output function determines the output of the neuron (1 if >0, else -1).\n",
    "outFun = lambda x: x > 0\n",
    "\n",
    "# TODO: Write a function that generates weights.\n",
    "def generateWeights(dims):\n",
    "    '''\n",
    "    Generates weights with the given bias for a number\n",
    "    of dimensions.\n",
    "    '''\n",
    "    W = rnd.rand(1, dims + 1)\n",
    "    W.shape = (1, dims + 1)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Testing the perceptron with a concrete example ##\n",
    "####################################################\n",
    "\n",
    "# Dimensions for our test.\n",
    "dims = 12 \n",
    "\n",
    "# Input is a row vector.\n",
    "D = np.append(1, rnd.rand(1, dims))\n",
    "D.shape = (1, dims + 1)\n",
    "\n",
    "#weights are stored in a vector\n",
    "W = generateWeights(dims)\n",
    "\n",
    "out = outFun(actFun(D, W))\n",
    "assert out == 1 or out == -1, \"The output has to be either 1 or -1, but was %d\" %out\n",
    "assert actFun(D, W).shape == (1, 1), \"The activation functions output should be one value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following $eval\\_network(t, D, W)$ function is used to measure the performance of your perceptron for the upcoming task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_network(t, D, W):\n",
    "    '''\n",
    "    This function takes the trained weights of a perceptron\n",
    "    and the input data (D) as well as the correct target values (t)\n",
    "    and computes the overall error rate of the perceptron.\n",
    "    '''\n",
    "    error = 0.0\n",
    "    size = np.max(D.shape)\n",
    "    for i in range(0, size):\n",
    "        out = outFun(actFun(D[i], W))\n",
    "        error = error + np.abs(t[i] - out)\n",
    "    # Normalize the error\n",
    "    return error/size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the above defined functions to train the perceptron to one of the following logical functions: AND, OR, NAND or NOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Now we train our perceptron! [ML-07, sl.33]\n",
    "#############################################\n",
    "\n",
    "# TODO write the update function for the weights dependent\n",
    "#      on epsilon, the target, the output and the input vector\n",
    "delta_fun = lambda eps,t,y,x: eps * (t-y) * x\n",
    "\n",
    "\n",
    "# TODO define suitable parameters for your problem\n",
    "eps = 0.1\n",
    "dims = 2\n",
    "training_size = 1250\n",
    "\n",
    "# TODO generate the weights\n",
    "W = generateWeights(dims)\n",
    "\n",
    "# Input\n",
    "# generate a list of truthvalue pairs\n",
    "D_i = rnd.rand(training_size, dims) > 0.5\n",
    "\n",
    "# pad the input with ones for the threshold/bias/w_0\n",
    "D = np.ones((training_size, dims+1))\n",
    "D[:,1:] = D_i \n",
    "D = np.matrix(D)\n",
    "\n",
    "# Example learn one of the logical functions: AND, OR, NAND, NOR\n",
    "op = lambda x1, x2: x1 and x2 #TODO change for other functions\n",
    "\n",
    "log_op = lambda row: op(row[0], row[1])\n",
    "labels = np.apply_along_axis(log_op, 1, D[:,1:])\n",
    "\n",
    "epochs = 20\n",
    "samp_size = 5\n",
    "\n",
    "for i in range(0,epochs):\n",
    "    #sample random from the training data\n",
    "    for idx in rnd.choice(range(0, training_size), samp_size):\n",
    "        y = outFun(actFun(D[idx], W))\n",
    "        W = W + delta_fun(eps, labels[idx], y, D[idx])\n",
    "\n",
    "\n",
    "#Print the overall performance of the Perceptron\n",
    "print(\"Overall error of the Perceptron: {:.2%}\".format(eval_network(labels, D, W)[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Space for complete own implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment : Sigmoid activation function & backpropagation delta funtion [ Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are first going to take the derivative of a famous activation function - the sigmoid function\n",
    "$$\\sigma(t)=\\frac{1}{1+e^{-t}}$$.\n",
    "This function is commonly used because of its nice analytical properties: it's $\\in[0,1]$, non-linear, strictly monotonous, continuous, differentiable and the derivative can be expressed in terms of the original function at the given point. This allows us to avoid redundant calculations. The sigmoid function is a special case of the more general *Logistic function}* which can be found in many different fields: Biology, chemistry, economics, demography and recently most prominently: Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the derivative $\\frac{d\\sigma}{dt}$ and (if possible) write the resulting expression in terms of $\\sigma(t)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\frac{d\\sigma}{dt}&=-e^{-t}*(-1)*\\frac{1}{(1+e^{-t})^2}\\\\\n",
    "&= \\frac{1}{1+e^{-t}} * \\frac{- 1 + 1 + e^{-t}}{1+e^{-t}} \\\\\n",
    "&= \\frac{1}{1+e^{-t}} * \\left(1 - \\frac{1}{1+e^{-t}}\\right) \\\\\n",
    "&= \\sigma(t)(1-\\sigma(t))\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPs can be regarded as a simple concatenation (and paralellization) of several perceptrons which each have a specified activation function $\\sigma$ and a set of weights $\\mathbf{w}_{ij}$. The idea that this can be done was discovered rather early on after the invention of the perceptron but people didn't really use it in practice that much because nobody really knew how to figure out the appropriate $\\mathbf{w}_{ij}$. The solution to this problem was the discovery of the backpropagation algorithm that consists of two steps: first propagating the input forward through the layers of the MLP and storing the intermediate results and then propagating the error backwards and adjusting the weights of the units accordingly.\n",
    "\n",
    "An updating rule for the output layer can be derived rather straightforward so we're going to let you do that. The rules for the intermediate layers can be derived very similarly and only require a slight shift in perspective - the mathematics for that are however not in the standard toolkit so we are going to omit the calculations and refer you to the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the least-squares approach to derive the updating rule, i.e. we want to minimize the Loss function\n",
    "$$L = \\frac{1}{2}(y-t)^2$$\n",
    "where t is the given (true) label from the dataset and y is the (single) output produced by the MLP. To find the weights that minimize this expression we want to take the derivative of $L$ w.r.t. $\\mathbf{w}_{i}$ where we are now going to assume that the $\\mathbf{w}_{i}$ are the ones directly before the output layer:\n",
    "$$y = \\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)$$\n",
    "Calculate $\\frac{dL}{d\\mathbf{w}_{i}}$.\n",
    "\n",
    "*Hint (This might only be helpful for some): $\\frac{dL}{d\\mathbf{w}_{i}}=\\frac{dL}{dy}\\frac{dy}{d\\mathbf{w}_{i}}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\frac{dL}{d\\mathbf{w}_{i}}&=\\frac{dL}{dy}\\frac{dy}{d\\mathbf{w}_{i}}\\\\\n",
    "&=(y-t)o_i\\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)\\left(1-\\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)\\right)\\\\\n",
    "&= (y-t)o_iy\\left(1-y\\right)\\\\\n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
