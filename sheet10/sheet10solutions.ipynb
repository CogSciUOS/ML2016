{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 26, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1:  [x Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Ultimate Dinosaur 3000 M4ze Xtrem!  [10 Points]\n",
    "\n",
    "In this assignment your task will be to use unsupervised learning methods and create the greatest dinosaur-maze simulation that the world has ever seen.\n",
    "\n",
    "here is some q-learning example, based on code from Moritz Meier, that I tried to put into a notebook ... maye that helps ... otherwise just ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    '''\n",
    "    Generate Field with the rewards\n",
    "    '''\n",
    "    field = np.zeros((x,y), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(x), rand.randint(y)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Softmax algorithm after the forumla: e^x/sum(e^x)\n",
    "    '''\n",
    "    e_x = np.exp(x) \n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, map_x, map_y):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field\n",
    "        self.q = np.zeros((len(ACTIONS), map_x, map_y))\n",
    "        self.gamma = learning_rate\n",
    "        # start on a random position in the field\n",
    "        self.pos = [np.random.randint(map_x), np.random.randint(map_y)]\n",
    "        # remember the map extend for further navigation\n",
    "        self.map_x = map_x\n",
    "        self.map_y = map_y\n",
    "    \n",
    "    def get_coordinates(self, choice):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain choice, depending\n",
    "        on the current position of the student\n",
    "        \"\"\"\n",
    "        cur_pos = self.pos.copy()\n",
    "        y_new = cur_pos[0]\n",
    "        x_new = cur_pos[1]\n",
    "        \n",
    "        if   choice == 'up'   : x_new -= 1 if x_new > 0 else 0\n",
    "        elif choice == 'down' : x_new += 1 if x_new < self.map_x else 0            \n",
    "        elif choice == 'left' : y_new -= 1 if y_new > 0 else 0                \n",
    "        elif choice == 'right': y_new += 1 if y_new < self.map_y else 0\n",
    "        else: raise ActionError('No such action:', name)\n",
    "            \n",
    "        return (x_new, y_new)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        # get qvals for the current state of the player\n",
    "        qvals = self.q[:,self.pos[0], self.pos[1]]\n",
    "        # select next action and exectue it\n",
    "        dist = softmax(np.asarray(qvals))\n",
    "        choice = np.random.choice(ACTIONS, p=dist)\n",
    "        \n",
    "        #receive the reward for this\n",
    "        rew = FIELD[self.pos[0],self.pos[1]]\n",
    "        \n",
    "        #observe new state\n",
    "        print(choice)\n",
    "        new_pos = self.get_coordinates(choice)\n",
    "        \n",
    "        #update q-value\n",
    "        self.q[choice, self.pos[0], self.pos[1]] = rew + self.gamma*max(self.q[:, new_pos[0], new_pos[1]])\n",
    "        \n",
    "        #update current position\n",
    "        self.pos = new_pos\n",
    "        \n",
    "        return self.q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# maze size an learning iterations\n",
    "m_x = 35\n",
    "m_y = 30\n",
    "\n",
    "steps = 100\n",
    "\n",
    "#Global variables\n",
    "\n",
    "ACTIONS = ['up','down','right','left']\n",
    "\n",
    "FIELD = generate_field(m_x, m_y, 4, 90)\n",
    "\n",
    "figure = plt.figure('Field')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(FIELD, interpolation='none')\n",
    "figure.canvas.draw()\n",
    "\n",
    "#generate player\n",
    "player = QLearning(0.9, m_x, m_y)\n",
    "\n",
    "# learning\n",
    "for i in range(steps):     \n",
    "    player_map = player.update()\n",
    "    \n",
    "print(player_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lala = np.zeros((4,10,12))\n",
    "\n",
    "pos = [3,4]\n",
    "print(pos)\n",
    "\n",
    "la = lala.copy()\n",
    "actions = ['up','down','right','left']\n",
    "#print(len(actions))\n",
    "\n",
    "#print(np.random.randint(9))\n",
    "\n",
    "#print((np.random.randint(45), np.random.randint(12)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
