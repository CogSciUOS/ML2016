{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 26, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1:  [x Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Ultimate Dinosaur 3000 M4ze Xtrem!  [10 Points]\n",
    "\n",
    "In this assignment we will have a look at the Q-Learning algorithm described in the lecture [ML-10 Reinforcement Learning]. For this we generate a field with random rewards. A learning agent is then exploring the field and learns the optimal path to navigate through it. The code below is again filled with some ``TODO``'s that should be filled by you in order to implement the Q-Learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    \"\"\"\n",
    "    Generate a random game field with rewards.\n",
    "    \n",
    "    Args:\n",
    "        x            x dimension of the field\n",
    "        y            y dimension of the field \n",
    "        num_rewards  the number of rewards that should be randomly placed\n",
    "        max_reward   the maximum reward that can be placed \n",
    "        \n",
    "    Returns:\n",
    "        A field with randomly initialized rewards, the rest of the \n",
    "        entries is zero\n",
    "    \"\"\"\n",
    "    field = np.zeros((x,y), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(x), rand.randint(y)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax algorithm after the forumla: e^x/sum(e^x)\n",
    "    \"\"\"\n",
    "    # TODO: implement the softmax function\n",
    "    e_x = np.exp(x) \n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, map_x, map_y):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        All q values are stored in self.q - this is an array that has\n",
    "        ACTIONS x map_x x map_y dimensions to store a value for each action\n",
    "        in each field. The starting position self.pos is randomly initialized.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate  the gamma in the lecture slides\n",
    "            map_x          x-dimension of the map\n",
    "            map_y          y-dimension of the map\n",
    "        \n",
    "        Returns:\n",
    "            An instance that can be used for QLearning on the field\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field\n",
    "        self.q = np.zeros((len(ACTIONS), map_x, map_y))\n",
    "        self.gamma = learning_rate\n",
    "        # start on a random position in the field\n",
    "        self.pos = [np.random.randint(map_x), np.random.randint(map_y)]\n",
    "        # remember the map extend for further navigation\n",
    "        self.map_x = map_x\n",
    "        self.map_y = map_y\n",
    "    \n",
    "    def get_coordinates(self, choice):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain choice, depending\n",
    "        on the current position of the learner. If the border is reached\n",
    "        the agent just stops there.\n",
    "        \n",
    "        Args:\n",
    "            choice   the action that should be performed (one of: 'up', 'down', ...)\n",
    "            \n",
    "        Returns:\n",
    "            the updated coordinates\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO return the right new coordinates depending on the position\n",
    "        y_new = self.pos[0]\n",
    "        x_new = self.pos[1]\n",
    "        \n",
    "        if   choice == 'up'   : x_new -= 1 if x_new > 0 else 0\n",
    "        elif choice == 'down' : x_new += 1 if x_new < self.map_x - 1 else 0            \n",
    "        elif choice == 'left' : y_new -= 1 if y_new > 0 else 0                \n",
    "        elif choice == 'right': y_new += 1 if y_new < self.map_y - 1 else 0\n",
    "        else: raise ActionError('No such action:', name)\n",
    "            \n",
    "        return (y_new, x_new)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Implementation of the update step. Closely follows the Algorithm described on\n",
    "        ML-10 Sl.18. Note that the you have attributes available as specified in the\n",
    "        __init__ method of this class, in addition to that is the FIELD variable that\n",
    "        stores the real field the agent is iterating about, as well as ACTIONS which\n",
    "        stores the available actions.\n",
    "        \"\"\"\n",
    "        # TODO:\n",
    "        # get the q-values for the current position of the player\n",
    "        qvals = self.q[:,self.pos[0], self.pos[1]]\n",
    "        #print(qvals)\n",
    "        \n",
    "        # select next action and exectue it\n",
    "        dist = softmax(np.asarray(qvals))\n",
    "        choice = np.random.choice(ACTIONS, p=dist)\n",
    "        new_pos = self.get_coordinates(choice)\n",
    "        print(new_pos)\n",
    "        \n",
    "        # TODO: \n",
    "        # receive the reward for this\n",
    "        rew = FIELD[new_pos[0],new_pos[1]]\n",
    "        print(rew)\n",
    "        \n",
    "        # TODO:\n",
    "        # observe the new state the player is now in an the index of the action\n",
    "        \n",
    "        choice_i = ACTIONS.index(choice)\n",
    "        \n",
    "        # TODO:\n",
    "        # update the q-value for the performed action\n",
    "        self.q[choice_i, self.pos[0], self.pos[1]] = rew + self.gamma*max(self.q[:, new_pos[0], new_pos[1]])\n",
    "        \n",
    "        # TODO:\n",
    "        # update the position of the player to the new field\n",
    "        self.pos = new_pos\n",
    "        print(self.q[choice_i, self.pos[0], self.pos[1]])\n",
    "        return self.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: choose maze size an learning iterations\n",
    "m_x = 10\n",
    "m_y = 10\n",
    "\n",
    "steps = 20\n",
    "\n",
    "# set global variables\n",
    "\n",
    "ACTIONS = ['up','down','right','left'] # those are the availabe actions for the qlearning\n",
    "\n",
    "FIELD = generate_field(m_x, m_y, 40, 90) # the field that is used for learning\n",
    "\n",
    "# Plotting the generated field (should be beautiful!)\n",
    "figure = plt.figure('Field')\n",
    "plt.axis('off')\n",
    "plt.imshow(FIELD, interpolation='none')\n",
    "figure.canvas.draw()\n",
    "\n",
    "# TODO: generate a QLearning instance with the right parameters\n",
    "player = QLearning(0.9, m_x, m_y)\n",
    "\n",
    "# now we perform steps many learning iterations on the field with\n",
    "# the generated QLearning instance\n",
    "for i in range(steps):     \n",
    "    player_map = player.update()\n",
    "    # FIXME: fancy plotting belongs here\n",
    "    \n",
    "print(player_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "maze = np.array([[0, 0, 1], [0, 0, 0]])\n",
    "actions = [lambda c : (c[0], c[1] + 1),\n",
    "           lambda c : (c[0], c[1] - 1),\n",
    "           lambda c : (c[0] - 1, c[1]),\n",
    "           lambda c : (c[0] + 1, c[1])]\n",
    "\n",
    "def move(pos, direction):\n",
    "    new_pos = actions[direction](pos)\n",
    "    for dim, c in enumerate(new_pos):\n",
    "        if c < 0 or c >= maze.shape[dim]:\n",
    "            raise ValueError('Action impossible.')\n",
    "    return new_pos\n",
    "\n",
    "# (Initialize parameters)\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize q(s, a) <- 0\n",
    "q = np.zeros((np.prod(maze.shape), len(actions)))\n",
    "\n",
    "# Observe current state s\n",
    "position = (0, 0)\n",
    "s = np.ravel_multi_index(position, maze.shape)\n",
    "\n",
    "# Repeat\n",
    "for iteration in range(10000):\n",
    "    # Select action a\n",
    "    a = np.random.randint(len(actions))\n",
    "    # Execute action a (if possible)\n",
    "    try:\n",
    "        position = move(position, a)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Receive reward r\n",
    "    r = maze[position]\n",
    "    # Observe new state s_n\n",
    "    s_n = np.ravel_multi_index(position, maze.shape)\n",
    "\n",
    "    # Update q(s, a)\n",
    "    q[s, a] = r + gamma * np.max(q[s_n, :])\n",
    "\n",
    "    # Update s\n",
    "    s = s_n\n",
    "\n",
    "print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
