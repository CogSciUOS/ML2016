{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 26, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1:  [x Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Ultimate Dinosaur 3000 M4ze Xtrem!  [10 Points]\n",
    "\n",
    "In this assignment your task will be to use unsupervised learning methods and create the greatest dinosaur-maze simulation that the world has ever seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    \"\"\"\n",
    "    Generate a random game field with rewards.\n",
    "    \n",
    "    Args:\n",
    "        x            x dimension of the field\n",
    "        y            y dimension of the field \n",
    "        num_rewards  the number of rewards that should be randomly placed\n",
    "        max_reward   the maximum reward that can be placed \n",
    "        \n",
    "    Returns:\n",
    "        A field with randomly initialized rewards, the rest of the \n",
    "        entries is zero\n",
    "    \"\"\"\n",
    "    field = np.zeros((x,y), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(x), rand.randint(y)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax algorithm after the forumla: e^x/sum(e^x)\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x) \n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, map_x, map_y):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate  the gamma in the lecture slides\n",
    "            map_x          x-dimension of the map\n",
    "            map_y          y-dimension of the map\n",
    "        \n",
    "        Returns:\n",
    "            An instance that can be used for QLearning on the field\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field\n",
    "        self.q = np.zeros((len(ACTIONS), map_x, map_y))\n",
    "        self.gamma = learning_rate\n",
    "        # start on a random position in the field\n",
    "        self.pos = [np.random.randint(map_x), np.random.randint(map_y)]\n",
    "        # remember the map extend for further navigation\n",
    "        self.map_x = map_x\n",
    "        self.map_y = map_y\n",
    "    \n",
    "    def get_coordinates(self, choice):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain choice, depending\n",
    "        on the current position of the learner. If the border is reached\n",
    "        the agent just stops there.\n",
    "        \"\"\"\n",
    "        y_new = self.pos[0]\n",
    "        x_new = self.pos[1]\n",
    "        \n",
    "        if   choice == 'up'   : x_new -= 1 if x_new > 0 else 0\n",
    "        elif choice == 'down' : x_new += 1 if x_new < self.map_x - 1 else 0            \n",
    "        elif choice == 'left' : y_new -= 1 if y_new > 0 else 0                \n",
    "        elif choice == 'right': y_new += 1 if y_new < self.map_y - 1 else 0\n",
    "        else: raise ActionError('No such action:', name)\n",
    "            \n",
    "        return (x_new, y_new)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Implementation of the update step. Closely follows the Algorithm described on\n",
    "        ML-10 Sl.18\n",
    "        \"\"\"\n",
    "        # get qvals for the current state of the player\n",
    "        qvals = self.q[:,self.pos[0], self.pos[1]]\n",
    "        # select next action and exectue it\n",
    "        dist = softmax(np.asarray(qvals))\n",
    "        choice = np.random.choice(ACTIONS, p=dist)\n",
    "        \n",
    "        #receive the reward for this\n",
    "        rew = FIELD[self.pos[0],self.pos[1]]\n",
    "        \n",
    "        #observe new state\n",
    "        new_pos = self.get_coordinates(choice)\n",
    "        choice_i = ACTIONS.index(choice)\n",
    "        \n",
    "        #update q-value\n",
    "        self.q[choice_i, self.pos[0], self.pos[1]] = rew + self.gamma*max(self.q[:, new_pos[0], new_pos[1]])\n",
    "        \n",
    "        #update current position\n",
    "        self.pos = new_pos\n",
    "        \n",
    "        return self.q       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# determine maze size an learning iterations\n",
    "m_x = 10\n",
    "m_y = 10\n",
    "\n",
    "steps = 100\n",
    "\n",
    "# set global variables\n",
    "\n",
    "ACTIONS = ['up','down','right','left']\n",
    "\n",
    "FIELD = generate_field(m_x, m_y, 4, 90)\n",
    "\n",
    "# Plotting the generated field (should be beautiful!)\n",
    "figure = plt.figure('Field')\n",
    "plt.axis('off')\n",
    "plt.imshow(FIELD, interpolation='none')\n",
    "figure.canvas.draw()\n",
    "\n",
    "#generate player\n",
    "player = QLearning(0.9, m_x, m_y)\n",
    "\n",
    "# let the player run through the field\n",
    "for i in range(steps):     \n",
    "    player_map = player.update()\n",
    "    # FIXME: fancy plotting belongs here\n",
    "    \n",
    "print(player_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "maze = np.array([[0, 0, 1], [0, 0, 0]])\n",
    "actions = [lambda c : (c[0], c[1] + 1),\n",
    "           lambda c : (c[0], c[1] - 1),\n",
    "           lambda c : (c[0] - 1, c[1]),\n",
    "           lambda c : (c[0] + 1, c[1])]\n",
    "\n",
    "def move(pos, direction):\n",
    "    new_pos = actions[direction](pos)\n",
    "    for dim, c in enumerate(new_pos):\n",
    "        if c < 0 or c >= maze.shape[dim]:\n",
    "            raise ValueError('Action impossible.')\n",
    "    return new_pos\n",
    "\n",
    "# (Initialize parameters)\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize q(s, a) <- 0\n",
    "q = np.zeros((np.prod(maze.shape), len(actions)))\n",
    "\n",
    "# Observe current state s\n",
    "position = (0, 0)\n",
    "s = np.ravel_multi_index(position, maze.shape)\n",
    "\n",
    "# Repeat\n",
    "for iteration in range(10000):\n",
    "    # Select action a\n",
    "    a = np.random.randint(len(actions))\n",
    "    # Execute action a (if possible)\n",
    "    try:\n",
    "        position = move(position, a)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Receive reward r\n",
    "    r = maze[position]\n",
    "    # Observe new state s_n\n",
    "    s_n = np.ravel_multi_index(position, maze.shape)\n",
    "\n",
    "    # Update q(s, a)\n",
    "    q[s, a] = r + gamma * np.max(q[s_n, :])\n",
    "\n",
    "    # Update s\n",
    "    s = s_n\n",
    "\n",
    "print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
