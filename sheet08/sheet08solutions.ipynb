{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 12, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Multilayer Perceptron (MLP) [10 Points]\n",
    "\n",
    "Last week you implemented a simple perceptron. We discussed that one can use multiple perceptrons to build a network. This week you will build your own MLP. Again the following code cells are just a guideline. If you feel like it, just follow the algorithm steps below in the empty cell - otherwise feel free to use our guided approach again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition\n",
    "Before we start, we need a problem to solve. In the following cell we first generate some three dimensional data (= $\\text{input_dim}$) between 0 and 1 and label all data according to a binary classification: If the sum of a data point's components is bigger than $0.7 \\cdot \\text{input_dim}$ the data point belongs to the first, otherwise to the second class.\n",
    "\n",
    "In the cell below we visualize the data set.\n",
    "\n",
    "**TODO (ahoereth)**: \n",
    "- Add padding between data for better results?\n",
    "- Describe what exactly we aim to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ND = 1000\n",
    "input_dim = 3\n",
    "D = np.random.rand(ND, input_dim)\n",
    "T = (np.sum(D, 1) > 0.7 * input_dim) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.figure().add_subplot(111, projection='3d').set_title('Data D')\n",
    "for t, c in [(0, 'cyan'), (1, 'orange')]:\n",
    "    x, y, z = tuple(zip(*D[T==t]))\n",
    "    plt.scatter(x, y, z, c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Multilayer Perceptron with Backpropagation\n",
    "\n",
    "We try to follow the definitions from the lecture (ML-07, Slide 46) closely:\n",
    "\n",
    "* Layers are numbered from $0$ (input layer) to $L_H + 1$ (output layer), such that $1 \\dots L_H$ are hidden layers.\n",
    "* Each layer has $N(i)$ neurons, numbered from $1 \\dots N(i)$.\n",
    "* $o_i(k)$ is the output of neuron $i$ in layer $k$.\n",
    "* $w_{ik}(m,n)$ is the weight between neuron $i$ in layer $m$ and neuron $k$ in layer $n$ (where for our case $m = n + 1$ holds).\n",
    "* The input to the MLP is $x \\in \\mathbb{R}^{d_{in}} = o(0)$, the output is $y \\in \\mathbb{R}^{d_{out}} = o(L_H + 1)$.\n",
    "* $\\epsilon$ is the learning rate.\n",
    "\n",
    "The algorithm you have to implement is now as follows:\n",
    "\n",
    "1. **Initialize your MLP.** Use as many input neurons as there are dimensions in the data. Input neurons always expect 1D input. Then create neurons for each hidden and the output layer. Each neuron in the hidden and output layers expects as many inputs as there are neurons in the layer before them.\n",
    "1. **Initialize the neurons' weights.** For each neuron in layers $1 \\dots L_H + 1$ initialize the weights to small random values (values between $0$ and $1$ are fine, but you are allowed to tweak the numbers around).\n",
    "1. **Implement the activation (feed-forward) step.**\n",
    "    1. Decompose the input into its components and pass them to the correct input neuron.\n",
    "    1. Each input neuron passes its unprocessed input to the next layer. That means each neuron in layer $1$ receives all outputs from each input layer as its own input.\n",
    "    $$o_i(0) = x_i$$\n",
    "    1. Calculate the weighted sums of their inputs and apply their activation function $\\sigma$ for each neuron in the layers $1 \\dots L_H + 1$. This is best done iteratively layer by layer, as each layer's input is the output of its preceding layer (Note: $w_{j0}(k,k)$ denotes the bias for neuron $j$ in layer $k$):\n",
    "    $$\\begin{align*}\n",
    "      o_j(k) = \\sigma\\left(w_{j0}(k,k)+\\sum\\limits_{i=1}^{N(k-1)} \n",
    "              o_i(k-1) w_{ji}(k,k-1)\\right)\n",
    "    \\end{align*}$$\n",
    "    with \n",
    "    $$\\sigma(x) = \\frac{1}{1 + \\exp{(-x)}}$$\n",
    "    1. The resulting $o_i(L_H+1)$ are the outputs $y_i$ for each output neuron $i$.\n",
    "1. **Implement the adaption (backpropagation) step.**\n",
    "    1. Compute the error between the target and output components to calculate the error signals $\\delta_i(L_H+1)$:\n",
    "    $$\\begin{align*}\n",
    "      \\delta_i(L_H + 1) &= o_i(L_H+1)\\ (1 - o_i(L_H+1))\\ (t_i - o_i(L_H + 1))\n",
    "    \\end{align*}$$\n",
    "    1. Calculate the error signals $\\delta_i(k)$ for each hidden layer $k$, starting with $k=L_H$ and going down to $k=1$.\n",
    "    $$\\begin{align*}\n",
    "    \\delta_i(k) &= o_i(k)\\ (1 - o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)\n",
    "    \\end{align*}$$\n",
    "    1. Adapt the weights for each neuron in the hidden and output layers.\n",
    "    $$\\Delta w_{ji}(k+1, k) = \\epsilon\\, \\delta_j(k+1)\\, o_i(k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "In the following you will be guided through implementing the above algorithm step by step. Instead of sticking to this guide, you are free to take a complete custom approach instead if you wish.\n",
    "\n",
    "We will basically take a bottom-up approach: Starting from an individual **perceptron** (or neruon), over a **layer of perceptrons** to a **multilayer perceptron** (or nerual network). Each of the 3 will be implemented as its own python *class*. Such a class basically defines a type of element, as a perceptron, which can be instantiated multiple times. You can think of such an instance as an individuum of a population (defined by a specific class). Each instance has specific methods which can be used to modify it -- again, taking the population reference, each puppy of the *puppy population* has the method *make_puppy_face()*.\n",
    "\n",
    "To guide you along all required classes and functions are outlined in valid python code with extensive comments. Each comment describes the arguments the specific method accepts (`Args`) and the values it is expected to return (`Returns`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def train_perceptron(perceptron, D, T, epochs, sample_size, verbose=True):\n",
    "    \"\"\"\n",
    "    Trains the perceptron over epochs epochs with sample_size \n",
    "    random samples drawn from D. No replacement is done in one epoch.\n",
    "    \n",
    "    Args:\n",
    "        perceptron  The perceptron. Must implement a function\n",
    "                    adaption(X, t) where X is a row of D and t\n",
    "                    is its label.\n",
    "        D           The data of size N x d where N is the\n",
    "                    number of samples and d is the number\n",
    "                    of dimensions.\n",
    "        T           The training labels. Iterable with\n",
    "                    N x do elements where N is the number\n",
    "                    of samples in D and do is the dimension\n",
    "                    of the perceptrons output.    \n",
    "        epochs      The number of training epochs.\n",
    "        sample_size The number of random samples per epoch.\n",
    "        verbose     Prints status messages if True (default).\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Training {}\\nEpochs: {}\\nSamples per Epoch: {}'.format(perceptron, epochs, sample_size))\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            sys.stdout.write(\"\\rEpoch {:5d}, {:7.2%}\".format(epoch + 1, (epoch + 1) / epochs))\n",
    "            sys.stdout.flush()\n",
    "        sample_indices = np.random.choice(range(len(D)), sample_size, replace=False)\n",
    "        for index in sample_indices:\n",
    "            x = D[index]\n",
    "            t = T[index]\n",
    "            perceptron.adaption(x, t)\n",
    "    if verbose:\n",
    "        print('\\nFinished.')\n",
    "\n",
    "def test_perceptron(perceptron, D, T, verbose=True):\n",
    "    \"\"\"\n",
    "    Tests the perceptron on all provided data.\n",
    "    \n",
    "    Args:\n",
    "        perceptron  The perceptron. Must implement a function\n",
    "                    activation(X) where X is a row of D.\n",
    "        D           The data of size N x d where N is the\n",
    "                    number of samples and d is the number\n",
    "                    of dimensions.\n",
    "        T           The training labels. Iterable with\n",
    "                    N x do elements where N is the number\n",
    "                    of samples in D and do is the dimension\n",
    "                    of the perceptrons output.\n",
    "        verbose     Prints status messages if True (default).\n",
    "    Returns:\n",
    "        The absolute error per output component.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Testing {}'.format(perceptron))\n",
    "    error = 0\n",
    "    for i, t in enumerate(T):\n",
    "        error += np.abs(t - perceptron.activation(D[i])) / len(D)\n",
    "    if verbose:\n",
    "        print('Total error:', error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an implementation of a simple perceptron. It provides an initialization method `__init__(dimensions, epsilon)`, an activation method `activation(X)` and an adaption method `adaption(X, t)` (the method `__repr__()` will be used if you `print(p)` the perceptron). You will use this perceptron as a starting point and overwrite certain methods with your own. This will be done by a programming principle called *inheritance*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BasePerceptron:\n",
    "    \"\"\"\n",
    "    A simple perceptron implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions=100, epsilon=0.03):\n",
    "        \"\"\"\n",
    "        Initializes the perceptron. Creates dimensions + 1\n",
    "        random weights (the additional weight is the bias.)\n",
    "\n",
    "        Args:\n",
    "            dimensions  the data dimensionality N\n",
    "            epsilon     the learning rate\n",
    "        \"\"\"\n",
    "        self.w = np.random.rand(dimensions + 1)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        The activation function. Prepends a 1 to X for the\n",
    "        bias and calculates the activation function of the \n",
    "        perceptron.\n",
    "\n",
    "        Args:\n",
    "            X   the data point, should be a numpy\n",
    "                array or a 1xN numpy matrix\n",
    "        Returns:\n",
    "            1   if the activation of X is bigger than 0\n",
    "            0   else\n",
    "        \"\"\"\n",
    "        return 1 if np.append(1, X) @ self.w > 0 else 0\n",
    "\n",
    "    def adaption(self, X, t):\n",
    "        \"\"\"\n",
    "        Trains the perceptron. Adjusts the weights according to \n",
    "        the learning rate and the error between the activation and the \n",
    "        label (delta).\n",
    "\n",
    "        Args:\n",
    "            X   the data point, should be a numpy\n",
    "                array or a 1xN numpy matrix\n",
    "            t   the label\n",
    "        \"\"\"\n",
    "        self.w += self.epsilon * (t - self.activation(X)) * np.append(1, X)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, len(self.w) - 1, self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell instantiates a BasePerceptron (`input_dim` is taken from above, where the data is created) and trains and tests it. The multilayer perceptron will behave exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a perceptron.\n",
    "epsilon = 0.03\n",
    "perceptron = BasePerceptron(input_dim, epsilon)\n",
    "\n",
    "# Train and test the perceptron for data D.\n",
    "train_perceptron(perceptron, D, T, epochs=100, sample_size=20)\n",
    "print()\n",
    "_ = test_perceptron(perceptron, D, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the MLP you need two basic building blocks: Input neurons and neurons for the hidden layers. Below there are two unfinished class definitions which inherit from the `BasePerceptron`. That means, that they have the exact same methods already available! This is cool, as you don't need to implement your own initialization or `__repr__` functions.\n",
    "\n",
    "Note however, that some methods are *redefined*. We adjusted the documentation for you, but we just copied the original code. \n",
    "\n",
    "One big difference between the `BasePerceptron` and the `InputPerceptron` and `ContinuousPerceptron` is, that the `BasePerceptron` is used by the functions provided above, while the `InputPerceptron` and `ContinuousPerceptron` are used by the `MultilayerPerceptron` which comes next.\n",
    "\n",
    "To implement these classes it might be needed that you get a better understanding of how they are used: If you don't figure out directly how they should work, continue with the `MultilayerPerceptron` below and come back whenever you need to.\n",
    "\n",
    "Below the `MultilayerPerceptron` there is a cell to test your implementation. You might want to change the number of neurons per (hidden) layer to see how your MLP handles it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The algorithm again**\n",
    "This is just a copy of the algorithm above so that you have it close by while implementing the next few classes.\n",
    "\n",
    "1. **Initialize your MLP.** Use as many input neurons as there are dimensions in the data. Input neurons always expect 1D input. Then create neurons for each hidden and the output layer. Each neuron in the hidden and output layers expects as many inputs as there are neurons in the layer before them.\n",
    "1. **Initialize the neurons' weights.** For each neuron in layers $1 \\dots L_H + 1$ initialize the weights to small random values (values between $0$ and $1$ are fine, but you are allowed to tweak the numbers around).\n",
    "1. **Implement the activation (feed-forward) step.**\n",
    "    1. Decompose the input into its components and pass them to the correct input neuron.\n",
    "    1. Each input neuron passes its unprocessed input to the next layer. That means each neuron in layer $1$ receives all outputs from each input layer as its own input.\n",
    "    $$o_i(0) = x_i$$\n",
    "    1. Calculate the weighted sums of their inputs and apply their activation function $\\sigma$ for each neuron in the layers $1 \\dots L_H + 1$. This is best done iteratively layer by layer, as each layer's input is the output of its preceding layer (Note: $w_{j0}(k,k)$ denotes the bias for neuron $j$ in layer $k$):\n",
    "    $$\\begin{align*}\n",
    "      o_j(k) = \\sigma\\left(w_{j0}(k,k)+\\sum\\limits_{i=1}^{N(k-1)} \n",
    "              o_i(k-1) w_{ji}(k,k-1)\\right)\n",
    "    \\end{align*}$$\n",
    "    with \n",
    "    $$\\sigma(x) = \\frac{1}{1 + \\exp{(-x)}}$$\n",
    "    1. The resulting $o_i(L_H+1)$ are the outputs $y_i$ for each output neuron $i$.\n",
    "1. **Implement the adaption (backpropagation) step.**\n",
    "    1. Compute the error between the target and output components to calculate the error signals $\\delta_i(L_H+1)$:\n",
    "    $$\\begin{align*}\n",
    "      \\delta_i(L_H + 1) &= o_i(L_H+1)\\ (1 - o_i(L_H+1))\\ (t_i - o_i(L_H + 1))\n",
    "    \\end{align*}$$\n",
    "    1. Calculate the error signals $\\delta_i(k)$ for each hidden layer $k$, starting with $k=L_H$ and going down to $k=1$.\n",
    "    $$\\begin{align*}\n",
    "    \\delta_i(k) &= o_i(k)\\ (1 - o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)\n",
    "    \\end{align*}$$\n",
    "    1. Adapt the weights for each neuron in the hidden and output layers.\n",
    "    $$\\Delta w_{ji}(k+1, k) = \\epsilon\\, \\delta_j(k+1)\\, o_i(k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InputPerceptron(BasePerceptron):\n",
    "    \"\"\"\n",
    "    InputPerceptron inherits all properties of the \n",
    "    BasePerceptron but implements a new activation function:\n",
    "    Instead of just using a threshold, it ignores its \n",
    "    weights and just passes on its input.\n",
    "    \"\"\"\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        The activation function for input perceptrons is\n",
    "        the identity function.\n",
    "\n",
    "        Args:\n",
    "            X           the data point\n",
    "        Returns:\n",
    "            X\n",
    "        \"\"\"\n",
    "        # TODO: Change this method.\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "class ContinuousPerceptron(BasePerceptron):\n",
    "    \"\"\"\n",
    "    ContinuousPerceptron inherits all properties of the \n",
    "    BasePerceptron but implements a new activation function:\n",
    "    Instead of just using a threshold, the continuous perceptron\n",
    "    uses a sigmoid function.\n",
    "    \"\"\"\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        The activation function. Prepends a 1 to X for the\n",
    "        bias and calculates the activation function of the \n",
    "        perceptron.\n",
    "\n",
    "        Args:\n",
    "            X           the data point, should be a numpy\n",
    "                        array or a 1xN numpy matrix\n",
    "        Returns:\n",
    "            1 / (1 + exp( -y ))\n",
    "            where y is the dot product of the weights and \n",
    "            the padded input.\n",
    "        \"\"\"\n",
    "        # TODO: Change this method.\n",
    "        return scipy.special.expit(self.w @ np.append(1, X))\n",
    "\n",
    "    def adaption(self, X, t):\n",
    "        \"\"\"\n",
    "        Trains the perceptron. Adjusts the weights according to \n",
    "        the learning rate and the error between the activation and the \n",
    "        label (delta).\n",
    "\n",
    "        Args:\n",
    "            X   the input to this perceptron\n",
    "            t   the delta\n",
    "        \"\"\"\n",
    "        # TODO: Change this method.\n",
    "        self.w += self.epsilon * t * np.append(1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultilayerPerceptron:\n",
    "    def __init__(self, dimensions=[2, 1, 1], epsilon=0.03):\n",
    "        \"\"\"\n",
    "        Initializes a multilayer perceptron.\n",
    "        \n",
    "        The argument dimensions denotes the number of neurons \n",
    "        per layer. The default is [2, 1, 1], which means that\n",
    "        there are 2 InputPerceptrons in the input layer, \n",
    "        1 ContinuousPerceptron in the first hidden layer and \n",
    "        also 1 ContinuousPerceptron in the output layer.\n",
    "        \n",
    "        Epsilon is passed on to all neurons in the MLP (except\n",
    "        for the input layer).\n",
    "        \n",
    "        The layers are stored as one list per layer containing\n",
    "        its neurons. All layers are stored in a list self.layers.\n",
    "        \n",
    "        Example:\n",
    "        self.layers = [[input neurons],\n",
    "                       [neurons of layer 1],\n",
    "                       [neurons of layer 2],\n",
    "                       [neurons of layer 3],\n",
    "                       [output neurons]]\n",
    "        \n",
    "        Args:\n",
    "            dimensions  A list of integers denoting the number \n",
    "                        of neurons per layer. The first integer\n",
    "                        denotes the number of input neurons,\n",
    "                        the last the number of output neurons.\n",
    "                        All integers in between denote neurons\n",
    "                        per hidden layer.\n",
    "            epsilon     The learning rate passed on to each neuron.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Create input layer.\n",
    "        self.layers.append([InputPerceptron(1, 0) for i in range(dimensions[0])])\n",
    "\n",
    "        # Generate hidden and output layers.\n",
    "        dim = len(self.layers[0])\n",
    "        for N in dimensions[1:]:\n",
    "            layer = [ContinuousPerceptron(dim, epsilon) for i in range(N)]\n",
    "            self.layers.append(layer)\n",
    "            dim = N\n",
    "\n",
    "        # Initialize outputs and deltas.\n",
    "        self.outputs = []\n",
    "        self.deltas = []\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        Implements the activation step (feed forward).\n",
    "        \n",
    "        Calculates the outputs for each layer by calling the \n",
    "        respective activation methods and stores the results\n",
    "        in a list of lists self.outputs such that\n",
    "        self.outputs[k][i] is the output of neuron self.layers[k][i]\n",
    "        where k is the layer and i is the i-th neuron of that layer.\n",
    "        \n",
    "        Args:\n",
    "            X   the input data\n",
    "        \n",
    "        Returns:\n",
    "            the network output, i.e. the outputs of the output layer.\n",
    "        \"\"\"\n",
    "        # Clear potentially stored outputs.\n",
    "        self.outputs = []\n",
    "\n",
    "        # Activate input layer and store its outputs.\n",
    "        layer_outputs = np.array([self.layers[0][i].activation(x) for i, x in enumerate(X)])\n",
    "\n",
    "        self.outputs.append(layer_outputs)\n",
    "\n",
    "        # Activate all other layers with the outputs from before.\n",
    "        for layer in self.layers[1:]:\n",
    "            layer_outputs = np.array([layer[j].activation(layer_outputs) for j in range(len(layer))])\n",
    "            self.outputs.append(layer_outputs)\n",
    "\n",
    "        # Return the last outputs for the output layer.\n",
    "        return np.copy(layer_outputs)\n",
    "\n",
    "    def adaption(self, X, t):\n",
    "        \"\"\"\n",
    "        Implements the adaption step (backpropagation).\n",
    "\n",
    "        Calls self.activation(X) to make sure the outputs are set\n",
    "        properly.\n",
    "        \n",
    "        Calculates the error signals for the output layer and \n",
    "        from there on calculates the error signals for each hidden \n",
    "        layer, starting with the last.\n",
    "        \n",
    "        Then adjusts each hidden and output neuron's weights by \n",
    "        calling the respective adaption functions.\n",
    "        \n",
    "        Args:\n",
    "            X   the data point\n",
    "            t   the label (must fit the number of output components)\n",
    "        \"\"\"\n",
    "        # Clear potentially stored deltas.\n",
    "        self.deltas = [0] * len(self.layers)\n",
    "\n",
    "        # Activate perceptron to figure out and store each\n",
    "        # neuron's output.\n",
    "        outputs = self.activation(X)\n",
    "        \n",
    "        # Calculate deltas for output layers.\n",
    "        self.deltas[-1] = (t - self.outputs[-1]) * self.outputs[-1] * (1 - self.outputs[-1])\n",
    "\n",
    "        # Calculate deltas for other layers.\n",
    "        for k in range(len(self.layers) - 2, 0, -1):\n",
    "            self.deltas[k] = [0] * len(self.layers[k])\n",
    "            for i in range(len(self.layers[k])):\n",
    "                # Collect weights from layer k + 1.\n",
    "                weights = np.array([n.w[i] for n in self.layers[k + 1]])\n",
    "                # Calculate deltas.\n",
    "                self.deltas[k][i] = weights @ self.deltas[k + 1] * self.outputs[k][i] * (1 - self.outputs[k][i])\n",
    "        \n",
    "        # Adapt weights.\n",
    "        for k, layer in enumerate(self.layers):\n",
    "            # Skip input layer.\n",
    "            if k == 0:\n",
    "                continue\n",
    "            for i, neuron in enumerate(layer):\n",
    "                neuron.adaption(self.outputs[k - 1], self.deltas[k][i])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'MultiLayerPerceptron({}, {})'.format([len(l) for l in self.layers], self.epsilon)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n\\t'.join([repr(self)] + [str(l) for l in self.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a multilayer perceptron.\n",
    "epsilon = 0.03\n",
    "output_dim = 1\n",
    "layers = [input_dim, 2, 2, output_dim]\n",
    "perceptron = MultilayerPerceptron(layers, epsilon)\n",
    "\n",
    "# Train and test the perceptron for data D.\n",
    "train_perceptron(perceptron, D, T, epochs=100, sample_size=20)\n",
    "print()\n",
    "_ = test_perceptron(perceptron, D, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: MLP and RBFN [10 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This exercise is aimed at deepening the understanding of Radial Basis Function Networks and how they relate to Multilayer Perceptrons. Not all of the answers can be found directly in the slides - so when answering the (more algorithmic) questions, first take a minute and think about how you would go about solving them and if nothing comes to mind search the internet for a little bit. If you are interested in a real life application of both algorithms and how they compare take a look at this paper: [Comparison between Multi-Layer Perceptron and Radial Basis Function Networks for Sediment Load Estimation in a Tropical Watershed](http://file.scirp.org/pdf/JWARP20121000014_80441700.pdf)\n",
    "\n",
    "![Schematic of a RBFN](RBFN.png)\n",
    "\n",
    "We have prepared a little example that shows how radial basis function approximation works in Python. This is not an example implementation of a RBFN but illustrates the work of the hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def func(x,y):\n",
    "    '''\n",
    "    This is the example function that should be fitted.\n",
    "    Its shape could be described as two peaks close to\n",
    "    each other - one going up, the other going down\n",
    "    '''\n",
    "    return (x + y) * np.exp(-4.0 * (x**2 + y**2))\n",
    "\n",
    "# number of training points (you may try different values here)\n",
    "training_size = 50\n",
    "\n",
    "# sample 'training_size' data points from the input space [-1,1]x[-1,1] ...\n",
    "x = uniform(-1.0, 1.0, size=training_size)\n",
    "y = uniform(-1.0, 1.0, size=training_size)\n",
    "\n",
    "# ... and compute function values for them.\n",
    "fvals = func(x, y)\n",
    "\n",
    "# get the aprroximation via RBF\n",
    "new_func = Rbf(x, y, fvals)\n",
    "\n",
    "\n",
    "# Plot both functions: \n",
    "# create a 100x100 grid of input values\n",
    "x_grid, y_grid = np.mgrid[-1:1:100j, -1:1:100j]\n",
    "\n",
    "plt.figure(\"Original Function\")\n",
    "# This plot represents the original function\n",
    "f_orig = func(x_grid, y_grid)\n",
    "plt.imshow(f_orig, extent=[-1,1,-1,1], cmap=plt.cm.jet)\n",
    "\n",
    "plt.figure(\"RBF Result\")\n",
    "# This plots the approximation of the original function by the RBF\n",
    "# if the plot looks strange try to run it again, the sampling\n",
    "# in the beginning is random\n",
    "f_new = new_func(x_grid, y_grid)\n",
    "plt.imshow(f_new, extent=[-1,1,-1,1], cmap=plt.cm.jet)\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "# scatter the datapoints that have been used by the RBF\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are radial basis functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial basis functions are all functions that fullfill the following criteria:\n",
    "\n",
    "The value of the function for a certain point depends only on the distance of that point to the origin or some other fixed center point. In mathematical formulation that spells out to: \n",
    "$\\phi (\\mathbf {x} )=\\phi (\\|\\mathbf {x} \\|)$  or  $\\phi (\\mathbf {x} ,\\mathbf {c} )=\\phi (\\|\\mathbf {x} -\\mathbf {c} \\|)$. Notice that it is not necessary (but most common) to use the norm as the measure of distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the structure of a RBFN? You may also use the notion from the above included picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBFN's are networks that contain only one hidden layer. The input is connected to all the hidden units. Each of the hidden units has a different radial basis function that is *sensitive* to ranges in the input domain. The output is then a linear combination of the outpus ot those functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is a RBFN trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all input data has to be normalized.\n",
    "\n",
    "Training a RBFN is a two-step process. First the functions in the hidden layer are initialized. This can be either done by sampling from the input data or by first performing a k-means clustering, where k is the number of nodes that have to be initialzed.\n",
    "\n",
    "The second step fits a linear model with coefficients $w_{i}$ to the hidden layer's outputs with respect to some objective function. The objective function depends on the task: it can be the least squares function, or the weights can be adapted by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to the Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do both models have in common? Where do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|RBFN                 |MLP                  | \n",
    "|---------------------|---------------------|\n",
    "| non-linear layered feedforward network|non-linear layered feedforward network| \n",
    "| hidden neurons use radial basis functions, output neurons use linear function| input, hidden and output-layer all use the same activation function| \n",
    "| universal approximator |   universal approximator |\n",
    "| learning usually affects only one or some RBF | learning affects many weights throught the network|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can classification in both networks be visualized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Classification](Solution_Classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When would you use a RBFN instead of a Multilayer Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBFNs are more robust to noise and should therefore be used when the data contains false-positives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
