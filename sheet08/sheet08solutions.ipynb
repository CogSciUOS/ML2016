{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise Sheet 08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 12, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 1: Multilayer Perceptron (MLP) [10 Points]\n",
    "\n",
    "Last week you implemented a simple perceptron. We discussed that one can use multiple perceptrons to build a network. This week you will build your own MLP. Again the following code cells are just a guideline. If you feel like it, just follow the algorithm steps below in the empty cell - otherwise feel free to use our guided approach again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Algorithm: Multilayer Perceptron with Backpropagation\n",
    "\n",
    "We try to follow the definitions from the lecture (ML-07, Slide 46) closely:\n",
    "\n",
    "* Layers are numbered from $0$ (input layer) to $L_H + 1$ (output layer), such that $1 \\dots L_H$ are hidden layers.\n",
    "* Each layer has $N(i)$ neurons, numbered from $1 \\dots N(i)$.\n",
    "* $o_i(k)$ is the output of neuron $i$ in layer $k$.\n",
    "* $w_{ik}(m,n)$ is the weight between neuron $i$ in layer $m$ and neuron $k$ in layer $n$ (where for our case $m = n + 1$ holds).\n",
    "* The input to the MLP is $x \\in \\mathbb{R}^{d_{in}} = o(0)$, the output is $y \\in \\mathbb{R}^{d_{out}} = o(L_H + 1)$.\n",
    "* $\\epsilon$ is the learning rate.\n",
    "\n",
    "The algorithm you have to implement is now as follows:\n",
    "\n",
    "1. **Initialize your MLP.** Use as many input neurons as there are dimensions in the data. Input neurons always expect 1D input. Then create neurons for each hidden and the output layer. Each neuron in the hidden and output layers expects as many inputs as there are neurons in the layer before them.\n",
    "1. **Initialize the neurons' weights.** For each neuron in layers $1 \\dots L_H + 1$ initialize the weights to small random values (values between $0$ and $1$ are fine, but you are allowed to tweak the numbers around).\n",
    "1. **Implement the activation (feed-forward) step.**\n",
    "    1. Decompose the input into its components and pass them to the correct input neuron.\n",
    "    1. Each input neuron passes its unprocessed input to the next layer. That means each neuron in layer $1$ receives all outputs from each input layer as its own input.\n",
    "    $$o_i(0) = x_i$$\n",
    "    1. Calculate the weighted sums of their inputs and apply their activation function $\\sigma$ for each neuron in the layers $1 \\dots L_H + 1$. This is best done iteratively layer by layer, as each layer's input is the output of its preceding layer (Note: $w_{j0}(k,k)$ denotes the bias for neuron $j$ in layer $k$):\n",
    "    $$\\begin{align*}\n",
    "      o_j(k) = \\sigma\\left(w_{j0}(k,k)+\\sum\\limits_{i=1}^{N(k-1)} \n",
    "              o_i(k-1) w_{ji}(k,k-1)\\right)\n",
    "    \\end{align*}$$\n",
    "    with \n",
    "    $$\\sigma(x) = \\frac{1}{1 + \\exp{(-x)}}$$\n",
    "    1. The resulting $o_i(L_H+1)$ are the outputs $y_i$ for each output neuron $i$.\n",
    "1. **Implement the adaption (backpropagation) step.**\n",
    "    1. Compute the error between the target and output components to calculate the error signals $\\delta_i(L_H+1)$:\n",
    "    $$\\begin{align*}\n",
    "      \\delta_i(L_H + 1) &= o_i(L_H+1)\\ (1 - o_i(L_H+1))\\ (t_i - o_i(L_H + 1))\n",
    "    \\end{align*}$$\n",
    "    1. Calculate the error signals $\\delta_i(k)$ for each hidden layer $k$, starting with $k=L_H$ and going down to $k=1$.\n",
    "    $$\\begin{align*}\n",
    "    \\delta_i(k) &= o_i(k)\\ (1 - o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)\n",
    "    \\end{align*}$$\n",
    "    1. Adapt the weights for each neuron in the hidden and output layers.\n",
    "    $$\\Delta w_{ji}(k+1, k) = \\epsilon\\, \\delta_j(k+1)\\, o_i(k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Implementation\n",
    "In the following you will be guided through implementing the above algorithm step by step. Instead of sticking to this guide, you are free to take a complete custom approach instead if you wish.\n",
    "\n",
    "We will basically take a bottom-up approach: Starting from an individual **perceptron** (or neruon), over a **layer of perceptrons** to a **multilayer perceptron** (or nerual network). Each of the 3 will be implemented as its own python *class*. Such a class basically defines a type of element which can be instantiated multiple times. You can think of the relation between such instances and their designated classes as individuum of a specific population. On the classes methods are defined, which can be used after having instanciated an instance in order to manipulate it or to make it perform a specific action -- again, taking the population reference, each puppy of a fictive *puppy population* would for example have the method `make_puppy_face`.\n",
    "\n",
    "To guide you along, all required classes and functions are outlined in valid python code with extensive comments. You just need to fill in the gaps. Each comment describes the arguments the specific method accepts (`Args`) and the values it is expected to return (`Returns`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Perceptron\n",
    "Simillar to last week you here need to implement a perceptron. But instead of applying it directly, we will do so in a class which is reusable for an theoretically infinite amount of perceptrons. For the perceptron you need to implement the following three things:\n",
    "\n",
    "##### Weight initialization\n",
    "\n",
    "**TODO (ahoereth)**: Explain.\n",
    "\n",
    "##### Forward-Propagation / Activation\n",
    "\n",
    "Calculate the weighted sums of a neuron's inputs and apply it's activation function $\\sigma$. The output vector $o$ of perceptron $j$ of layer $k$ given an input (the output of the previous layer) $x$ in a neural network is given by the following formula. Note: $N$ gives the number of values of a given vector, $w_{j,0}(k)$ specifies the bias of perceptron $j$ in layer $k$ and $w_{j,1...N(x)}(k)$ the other weights of perceptron $j$ in layer $k$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "  o_{k,j}(x) = \\sigma\\left(w_{j,0}(k)+\\sum\\limits_{i=1}^{N(x)} x_i w_{j,i}(k)\\right)\n",
    "\\end{align*}$$\n",
    "with \n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp{(-x)}}$$\n",
    "\n",
    "Think of weights $w(k)$ as a matrix being located inbetween the layer $k$ and the layer located *to its left* in the network. So values flowing from layer $k-1$ to layer $k$ are weighted by the values of $w(k)$.\n",
    "\n",
    "##### Back-Propagation / Adaptation\n",
    "The perceptron itself does not need to worry about calculating its $\\Delta$ weight adaptation value. Instead, it is provided by the network. Still, the perceptron needs to adjust its weights by the specified delta weighted by a given learning rate:\n",
    "\n",
    "$$\\Delta w_{j,i}(k) = \\epsilon\\, \\delta_j(k+1)\\, o_i(k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "# sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"Single neuron handling its own weights and bias.\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, act_func=sigmoid):\n",
    "        \"\"\"Initialize a new neuron with its weights and bias.\n",
    "\n",
    "        Args:\n",
    "            dim_in: Dimensionality of the data coming into\n",
    "                this perceptron. In a network of perceptrons\n",
    "                this basically represents the number of neurons\n",
    "                in the layer before this neuron's layer. Used\n",
    "                for generating the perceptron's weights vector,\n",
    "                which not only includes one weight per input but\n",
    "                also an additional bias weight.\n",
    "            act_fun: Function to apply on activation.\n",
    "        \"\"\"\n",
    "        self.act_func = act_func\n",
    "        # self.weights = []  # TODO\n",
    "        self.weights = np.random.normal(size=dim_in + 1)\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate this neuron with a specific input.\n",
    "\n",
    "        Calculate the weighted sum of inputs and apply the\n",
    "        activation function.\n",
    "\n",
    "        Args:\n",
    "            x: Vector of input values.\n",
    "\n",
    "        Returns:\n",
    "            A real number representing the perceptron's\n",
    "            activation after calculating the weighted sum\n",
    "            of inputs and applying the perceptron's\n",
    "            activation function.\n",
    "        \"\"\"\n",
    "        # return self.act_func(0)  # TODO\n",
    "        return self.act_func(self.weights @ np.append(1, x))\n",
    "\n",
    "    def adapt(self, x, delta, rate=0.03):\n",
    "        \"\"\"Adapt this neuron's weights by a specific delta.\n",
    "\n",
    "        Args:\n",
    "            x: Vector of input values.\n",
    "            delta: Weight adaptation delta value.\n",
    "            rate: Learning rate.\n",
    "        \"\"\"\n",
    "        # self.weights += 0  # TODO\n",
    "        self.weights += rate * delta * np.append(1, x)\n",
    "\n",
    "\n",
    "# TODO (ahoereth): Add asserts testing the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### PerceptronLayer\n",
    "A PerceptronLayer basically is a combination of multiple `Perceptron` instances. It mainly is concerened with passing input and delta values to its individual neurons. No formulas required here, only a couple of loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PerceptronLayer:\n",
    "    \"\"\"Layer of multiple neurons.\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, act_func=sigmoid):\n",
    "        \"\"\"Initialize the layer as a list of individual neurons.\n",
    "\n",
    "        A layer contains as many neurons as it has outputs, each\n",
    "        neuron has as many input weights (+ bias) as the layer has inputs.\n",
    "        \n",
    "        Args:\n",
    "            dim_in: Dimensionality of the expected input values,\n",
    "                also the size of the previous layer of a neural network.\n",
    "            dim_out: Dimensionality of the output, also the requested \n",
    "                amount of in this layer and the input dimension of the\n",
    "                next layer.\n",
    "            act_func: Activation function to use in each perceptron of\n",
    "                this layer.\n",
    "        \"\"\"\n",
    "        # self.perceptrons = []  # TODO\n",
    "        self.perceptrons = [Perceptron(dim_in, act_func)\n",
    "                            for _ in range(dim_out)]\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate this layer by activating each individual neuron.\n",
    "        \n",
    "        Args:\n",
    "            x: Vector of input values.\n",
    "            \n",
    "        Retuns:\n",
    "            Vector of output values which can be used as input to\n",
    "            another PerceptronLayer instance.\n",
    "        \"\"\"\n",
    "        # return np.asarray([])  # TODO\n",
    "        return np.asarray([p.activate(x) for p in self.perceptrons])\n",
    "\n",
    "    def adapt(self, x, deltas, rate=0.03):\n",
    "        \"\"\"Adapt this layer by adapting each individual neuron.\n",
    "        \n",
    "        Args:\n",
    "            x: Vector of input values.\n",
    "            deltas: Vector of delta values.\n",
    "            rate: Learning rate.\n",
    "        \"\"\"\n",
    "        # pass  # TODO\n",
    "        for perceptron, delta in zip(self.perceptrons, deltas):\n",
    "            perceptron.adapt(x, delta, rate)\n",
    "\n",
    "    def get_weights_matrix(self):\n",
    "        \"\"\"Helper function for getting this layer's weight matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Numpy array of all the weights for this perceptron layer.\n",
    "        \"\"\"\n",
    "        return np.asarray([p.weights for p in self.perceptrons]).T\n",
    "\n",
    "\n",
    "# TODO (ahoereth): Add asserts testing the perceptron layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### MultilayerPerceptron\n",
    "\n",
    "##### Forward-Propagation / Activation\n",
    "Propagate the input value $x$ through each layer of the network, employing the output of the previous layer as input to the next layer.\n",
    "\n",
    "##### Back-Propagation / Adaptation\n",
    "This is the most complex step of the problem. Split into 3 parts:\n",
    "\n",
    "**TODO (ahoereth)**: This explanation needs more work.\n",
    "\n",
    "1. Compute individual output values for each layer -- similar to the forward-propagation step above, but we need to keep track of the intermediate results.\n",
    "\n",
    "2. Calculate error signals $\\delta_i$ for all layers $k$:\n",
    "\n",
    "   $$\\delta_i(k) = o_i(k)\\ (1 - o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)$$\n",
    "\n",
    "3. Adapt the weights of each layer by $\\Delta$.\n",
    "\n",
    "$$\\Delta(k+1) = \\epsilon\\, \\delta_j(k+1)\\, o_i(k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron:\n",
    "    \"\"\"Network of perceptrons, also a set of multiple perceptron layers.\"\"\"\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        \"\"\"Initialize a new network, madeup of individual PerceptronLayers.\n",
    "\n",
    "        Args:\n",
    "            *layers: Arbritrarily many PerceptronLayer instances.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate network and return the last layer's output.\n",
    "\n",
    "        Args:\n",
    "            x: Vector of input values.\n",
    "\n",
    "        Returns:\n",
    "            Vector of output values from the last layer of the network\n",
    "            after propagating forward through the network.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.activate(x)\n",
    "        return x\n",
    "\n",
    "    def adapt(self, x, t, rate=0.03):\n",
    "        \"\"\"Adapt the whole network given an input and expected output.\n",
    "\n",
    "        Args:\n",
    "            x: Vector of input values.\n",
    "            t: Vector of target values (expected outputs).\n",
    "            rate: Learning rate.\n",
    "        \"\"\"\n",
    "        # Activate each layer and collect intermediate outputs.\n",
    "        # outputs = []  # TODO\n",
    "        outputs = [x]\n",
    "        for layer in self.layers:\n",
    "            outputs.append(layer.activate(outputs[-1]))\n",
    "\n",
    "        # Calculate error between t and network output.\n",
    "        # e = []  # TODO\n",
    "        e = t - outputs[-1]\n",
    "\n",
    "        # Backpropagate error through the network and adapt each layer.\n",
    "        # TODO (ahoereth): Get rid of zip here because its \n",
    "        #     difficult to grasp not only for python beginners.\n",
    "        layers = list(zip(self.layers, outputs[:-1], outputs[1:]))\n",
    "        for layer, x, y in reversed(layers):\n",
    "            delta = (y * (1 - y)) * e\n",
    "            weights = layer.get_weights_matrix()\n",
    "            # TODO (ahoereth): The following line for many is hard and\n",
    "            #     therefore the need to remove the bias should be\n",
    "            #     somehow explained.\n",
    "            e = (weights @ delta)[1:]\n",
    "            layer.adapt(x, delta, rate)\n",
    "\n",
    "\n",
    "# TODO (ahoereth): Add asserts testing the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Problem Definition\n",
    "Before we start, we need a problem to solve. In the following cell we first generate some three dimensional data (= $\\text{input_dim}$) between 0 and 1 and label all data according to a binary classification: If the sum of a data point's components is bigger than $0.7 \\cdot \\text{input_dim}$ the data point belongs to the first, otherwise to the second class.\n",
    "\n",
    "In the cell below we visualize the data set.\n",
    "\n",
    "**TODO (ahoereth)**: \n",
    "- Add padding between data for better results?\n",
    "- Describe what exactly we aim to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def uniform(a, b, n=1):\n",
    "    \"\"\"Returns n floats uniformly distributed between a and b.\"\"\"\n",
    "    return (b - a) * np.random.random_sample(n) + a\n",
    "\n",
    "n = 1000\n",
    "radius = 5\n",
    "r = np.append(uniform(0, radius * .5, n // 2),\n",
    "              uniform(radius * .7, radius, n // 2))\n",
    "angle = uniform(0, 2*np.pi, n)\n",
    "x = r * np.sin(angle) + uniform(-radius, radius, n)\n",
    "y = r * np.cos(angle) + uniform(-radius, radius, n)\n",
    "inputs = np.vstack((x, y)).T\n",
    "targets = np.less(np.linalg.norm(inputs, axis=1), radius * .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.suptitle('Labeled Data')\n",
    "plt.scatter(*inputs.T, 2, c=targets, cmap='RdYlBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MLP = MultilayerPerceptron(\n",
    "    PerceptronLayer(2, 4),\n",
    "    PerceptronLayer(4, 2),\n",
    "    PerceptronLayer(2, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "EPOCHS = 100000\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    s = np.random.randint(0, len(targets))\n",
    "    MLP.adapt(inputs[s], targets[s])\n",
    "\n",
    "    if epoch % 2500 == 0:\n",
    "        outputs = np.squeeze([MLP.activate(x) for x in inputs])\n",
    "        predictions = np.round(outputs)\n",
    "        accuracy = np.sum(predictions == targets) / len(targets)\n",
    "        max_accuracy = max_accuracy if accuracy < max_accuracy else accuracy\n",
    "        sys.stdout.write('\\rAccuracy at epoch {:6d}: {:.4f}%. Max accuracy: {:.4f}%'\n",
    "                         .format(epoch, accuracy, max_accuracy))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle('Evaluation')\n",
    "ax = plt.subplot(2,2,1)\n",
    "ax.scatter(*inputs.T, 2, c=outputs, cmap='RdYlBu')\n",
    "ax.set_title('Continious Classification')\n",
    "ax = plt.subplot(2,2,2)\n",
    "ax.set_title('Discretized Classification')\n",
    "ax.scatter(*inputs.T, 2, c=np.round(outputs), cmap='RdYlBu')\n",
    "ax = plt.subplot(2,2,3)\n",
    "ax.set_title('Labels')\n",
    "ax.scatter(*inputs.T, 2, c=targets, cmap='RdYlBu')\n",
    "ax = plt.subplot(2,2,4)\n",
    "ax.set_title('Wrong Classifications')\n",
    "ax.scatter(*inputs.T, 2, c=(targets!=np.round(outputs)), cmap='OrRd');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 2: MLP and RBFN [10 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "This exercise is aimed at deepening the understanding of Radial Basis Function Networks and how they relate to Multilayer Perceptrons. Not all of the answers can be found directly in the slides - so when answering the (more algorithmic) questions, first take a minute and think about how you would go about solving them and if nothing comes to mind search the internet for a little bit. If you are interested in a real life application of both algorithms and how they compare take a look at this paper: [Comparison between Multi-Layer Perceptron and Radial Basis Function Networks for Sediment Load Estimation in a Tropical Watershed](http://file.scirp.org/pdf/JWARP20121000014_80441700.pdf)\n",
    "\n",
    "![Schematic of a RBFN](RBFN.png)\n",
    "\n",
    "We have prepared a little example that shows how radial basis function approximation works in Python. This is not an example implementation of a RBFN but illustrates the work of the hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def func(x,y):\n",
    "    '''\n",
    "    This is the example function that should be fitted.\n",
    "    Its shape could be described as two peaks close to\n",
    "    each other - one going up, the other going down\n",
    "    '''\n",
    "    return (x + y) * np.exp(-4.0 * (x**2 + y**2))\n",
    "\n",
    "# number of training points (you may try different values here)\n",
    "training_size = 50\n",
    "\n",
    "# sample 'training_size' data points from the input space [-1,1]x[-1,1] ...\n",
    "x = uniform(-1.0, 1.0, size=training_size)\n",
    "y = uniform(-1.0, 1.0, size=training_size)\n",
    "\n",
    "# ... and compute function values for them.\n",
    "fvals = func(x, y)\n",
    "\n",
    "# get the aprroximation via RBF\n",
    "new_func = Rbf(x, y, fvals)\n",
    "\n",
    "\n",
    "# Plot both functions: \n",
    "# create a 100x100 grid of input values\n",
    "x_grid, y_grid = np.mgrid[-1:1:100j, -1:1:100j]\n",
    "\n",
    "plt.figure(\"Original Function\")\n",
    "# This plot represents the original function\n",
    "f_orig = func(x_grid, y_grid)\n",
    "plt.imshow(f_orig, extent=[-1,1,-1,1], cmap=plt.cm.jet)\n",
    "\n",
    "plt.figure(\"RBF Result\")\n",
    "# This plots the approximation of the original function by the RBF\n",
    "# if the plot looks strange try to run it again, the sampling\n",
    "# in the beginning is random\n",
    "f_new = new_func(x_grid, y_grid)\n",
    "plt.imshow(f_new, extent=[-1,1,-1,1], cmap=plt.cm.jet)\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "# scatter the datapoints that have been used by the RBF\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Radial Basis Function Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### What are radial basis functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Radial basis functions are all functions that fullfill the following criteria:\n",
    "\n",
    "The value of the function for a certain point depends only on the distance of that point to the origin or some other fixed center point. In mathematical formulation that spells out to: \n",
    "$\\phi (\\mathbf {x} )=\\phi (\\|\\mathbf {x} \\|)$  or  $\\phi (\\mathbf {x} ,\\mathbf {c} )=\\phi (\\|\\mathbf {x} -\\mathbf {c} \\|)$. Notice that it is not necessary (but most common) to use the norm as the measure of distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### What is the structure of a RBFN? You may also use the notion from the above included picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "RBFN's are networks that contain only one hidden layer. The input is connected to all the hidden units. Each of the hidden units has a different radial basis function that is *sensitive* to ranges in the input domain. The output is then a linear combination of the outpus ot those functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### How is a RBFN trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note: all input data has to be normalized.\n",
    "\n",
    "Training a RBFN is a two-step process. First the functions in the hidden layer are initialized. This can be either done by sampling from the input data or by first performing a k-means clustering, where k is the number of nodes that have to be initialzed.\n",
    "\n",
    "The second step fits a linear model with coefficients $w_{i}$ to the hidden layer's outputs with respect to some objective function. The objective function depends on the task: it can be the least squares function, or the weights can be adapted by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Comparison to the Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### What do both models have in common? Where do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "|RBFN                 |MLP                  | \n",
    "|---------------------|---------------------|\n",
    "| non-linear layered feedforward network|non-linear layered feedforward network| \n",
    "| hidden neurons use radial basis functions, output neurons use linear function| input, hidden and output-layer all use the same activation function| \n",
    "| universal approximator |   universal approximator |\n",
    "| learning usually affects only one or some RBF | learning affects many weights throught the network|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### How can classification in both networks be visualized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![Classification](Solution_Classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### When would you use a RBFN instead of a Multilayer Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "RBFNs are more robust to noise and should therefore be used when the data contains false-positives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
