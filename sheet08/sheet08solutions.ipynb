{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 12, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Multilayer Perceptron (MLP) [10 Points]\n",
    "\n",
    "Last week you implemented a simple perceptron. We discussed that one can use multiple perceptrons to build a network. This week you will build your own MLP. Again the following code cells are just a guideline. If you feel like it, just follow the algorithm steps below in the empty cell - otherwise feel free to use our guided approach again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Multilayer Perceptron with Backpropagation\n",
    "\n",
    "We try to follow the definitions from the lecture (ML-07, Slide 46) closely:\n",
    "\n",
    "* Layers are numbered from $0$ (input layer) to $L_H + 1$ (output layer), such that $1 \\dots L_H$ are hidden layers.\n",
    "* Each layer has $N(i)$ neurons, numbered from $1 \\dots N(i)$.\n",
    "* $o_i(k)$ is the output of neuron $i$ in layer $k$.\n",
    "* $w_{ik}(m,n)$ is the weight between neuron $i$ in layer $m$ and neuron $k$ in layer $n$ (where for our case $m = n + 1$ holds).\n",
    "* The input to the MLP is $x \\in \\mathbb{R}^{d_{in}} = o(0)$, the output is $y \\in \\mathbb{R}^{d_{out}} = o(L_H + 1)$.\n",
    "* $\\epsilon$ is the learning rate.\n",
    "\n",
    "The algorithm you have to implement is now as follows:\n",
    "\n",
    "1. **Initialize your MLP.** Use as many input neurons as there are dimensions in the data. Input neurons always expect 1D input. Then create neurons for each hidden and the output layer. Each neuron in the hidden and output layers expects as many inputs as there are neurons in the layer before them.\n",
    "1. **Initialize the neurons' weights.** For each neuron in layers $1 \\dots L_H + 1$ initialize the weights to small random values (values between $0$ and $1$ are fine, but you are allowed to tweak the numbers around).\n",
    "1. **Implement the activation (feed-forward) step.**\n",
    "    1. Decompose the input into its components and pass them to the correct input neuron.\n",
    "    1. Each input neuron passes its unprocessed input to the next layer. That means each neuron in layer $1$ receives all outputs from each input layer as its own input.\n",
    "    $$o_i(0) = x_i$$\n",
    "    1. Calculate the weighted sums of their inputs and apply their activation function $\\sigma$ for each neuron in the layers $1 \\dots L_H + 1$. This is best done iteratively layer by layer, as each layer's input is the output of its preceding layer (Note: $w_{j0}(k,k)$ denotes the bias for neuron $j$ in layer $k$):\n",
    "    $$\\begin{align*}\n",
    "      o_j(k) = \\sigma\\left(w_{j0}(k,k)+\\sum\\limits_{i=1}^{N(k-1)} \n",
    "              o_i(k-1) w_{ji}(k,k-1)\\right)\n",
    "    \\end{align*}$$\n",
    "    with \n",
    "    $$\\sigma(x) = \\frac{1}{1 + \\exp{(-x)}}$$\n",
    "    1. The resulting $o_i(L_H+1)$ are the outputs $y_i$ for each output neuron $i$.\n",
    "1. **Implement the adaption (backpropagation) step.**\n",
    "    1. Compute the error between the target and output components to calculate the error signals $\\delta_i(L_H+1)$ by multiplying them with the derivative $\\sigma^\\prime(x)$:\n",
    "    $$\\begin{align*}\n",
    "      \\delta_i(L_H + 1) &= \\sigma^\\prime(o_i(L_H + 1))\\ (t_i - o_i(L_H + 1)) \\\\\n",
    "                        &= o_i(L_H + 1)\\ (1 - o_i(L_H + 1))\\ (t_i - o_i(L_H + 1))\n",
    "    \\end{align*}$$\n",
    "    1. Calculate the the error signals $\\delta_i(k)$ for each hidden layer $k$, starting with $k=L_H$ and going down to $k=1$.\n",
    "    $$\\begin{align*}\n",
    "    \\delta_i(k) &= \\sigma^\\prime(o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1) \\\\\n",
    "                &= o_i(k)\\ (1 - o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)\n",
    "    \\end{align*}$$\n",
    "    1. Adapt the weights for each neuron in the hidden and output layers.\n",
    "    $$\\Delta w_{ji}(k+1, k) = \\epsilon \\delta_j(k+1) o_i(k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Approach\n",
    "\n",
    "Use this to implement and test your own version of an MLP. You are of course allowed to structure the code into more cells, use code from below etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Approach\n",
    "In the following there are multiple cells with code, most of them are there to make the exercise easier. All functions and methods come with a docstring (roughly inspired by the [Google guidelines](https://google.github.io/styleguide/pyguide.html?showone=Comments#Comments) for Python), read them to get a better idea of what the functions are doing. They usually explain what the function does, what input argumens (`Args:`) it expects and what return values (`Returns:`) you can use.\n",
    "\n",
    "The first cell generates random three dimensional data (= $\\text{input_dim}$) between 0 and 1 and labels all data according to a binary classification: If the sum of a data point's components is bigger than $0.7 \\cdot \\text{input_dim}$ the data point belongs to one class, otherwise to the other. \n",
    "\n",
    "The second cell is a simple plot to give you an idea of how this looks like. By now you probably already figured out whether this task can be done with a simple perceptron or not. If you didn't, run the first and second cell and take a closer look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some data.\n",
    "ND = 1000\n",
    "input_dim = 3\n",
    "D = np.random.rand(ND, input_dim)\n",
    "# Label data: sum should be > 0.7 * dim.\n",
    "T = (np.sum(D, 1) > 0.7 * input_dim) * 1\n",
    "\n",
    "# Generate another set of data: XOR (one value above \n",
    "# 0.5, but not the other)\n",
    "D2 = np.random.rand(ND, 2)\n",
    "T2 = np.logical_xor(D2[:,0] > 0.5, D2[:,1] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.figure().add_subplot(111, projection='3d')\n",
    "for t, c in [(0, 'cyan'), (1, 'orange')]:\n",
    "    x, y, z = tuple(zip(*D[T==t]))\n",
    "    plt.scatter(x, y, zs=z, c=c)\n",
    "plt.gcf().canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides two functions: One to train your perceptron, one to test it. They will be used further below, but their explanations will give you an idea of what you are supposed to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def train_perceptron(perceptron, D, T, epochs, sample_size, verbose=True):\n",
    "    \"\"\"\n",
    "    Trains the perceptron over epochs epochs with sample_size \n",
    "    random samples drawn from D. No replacement is done in one epoch.\n",
    "    \n",
    "    Args:\n",
    "        perceptron  The perceptron. Must implement a function\n",
    "                    adaption(X, t) where X is a row of D and t\n",
    "                    is its label.\n",
    "        D           The data of size N x d where N is the\n",
    "                    number of samples and d is the number\n",
    "                    of dimensions.\n",
    "        T           The training labels. Iterable with\n",
    "                    N x do elements where N is the number\n",
    "                    of samples in D and do is the dimension\n",
    "                    of the perceptrons output.    \n",
    "        epochs      The number of training epochs.\n",
    "        sample_size The number of random samples per epoch.\n",
    "        verbose     Prints status messages if True (default).\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Training {}\\nEpochs: {}\\nSamples per Epoch: {}'.format(perceptron, epochs, sample_size))\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            sys.stdout.write(\"\\rEpoch {:5d}, {:7.2%}\".format(epoch + 1, (epoch + 1) / epochs))\n",
    "            sys.stdout.flush()\n",
    "        sample_indices = np.random.choice(range(len(D)), sample_size, replace=False)\n",
    "        for index in sample_indices:\n",
    "            x = D[index]\n",
    "            t = T[index]\n",
    "            perceptron.adaption(x, t)\n",
    "    if verbose:\n",
    "        print('\\nFinished.')\n",
    "\n",
    "def test_perceptron(perceptron, D, T, verbose=True):\n",
    "    \"\"\"\n",
    "    Tests the perceptron on all provided data.\n",
    "    \n",
    "    Args:\n",
    "        perceptron  The perceptron. Must implement a function\n",
    "                    activation(X) where X is a row of D.\n",
    "        D           The data of size N x d where N is the\n",
    "                    number of samples and d is the number\n",
    "                    of dimensions.\n",
    "        T           The training labels. Iterable with\n",
    "                    N x do elements where N is the number\n",
    "                    of samples in D and do is the dimension\n",
    "                    of the perceptrons output.\n",
    "        verbose     Prints status messages if True (default).\n",
    "    Returns:\n",
    "        The absolute error per output component.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Testing {}'.format(perceptron))\n",
    "    error = 0\n",
    "    for i, t in enumerate(T):\n",
    "        error += np.abs(t - perceptron.activation(D[i])) / len(D)\n",
    "    if verbose:\n",
    "        print('Total error:', error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an implementation of a simple perceptron. It provides an initialization method `__init__(dimensions, epsilon)`, an activation method `activation(X)` and an adaption method `adaption(X, t)` (the method `__repr__()` will be used if you `print(p)` the perceptron). You will use this perceptron as a starting point and overwrite certain methods with your own. This will be done by a programming principle called *inheritance*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BasePerceptron:\n",
    "    \"\"\"\n",
    "    A simple perceptron implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions=100, epsilon=0.03):\n",
    "        \"\"\"\n",
    "        Initializes the perceptron. Creates dimensions + 1\n",
    "        random weights (the additional weight is the bias.)\n",
    "\n",
    "        Args:\n",
    "            dimensions  the data dimensionality N\n",
    "            epsilon     the learning rate\n",
    "        \"\"\"\n",
    "        self.w = np.random.rand(dimensions + 1)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        The activation function. Prepends a 1 to X for the\n",
    "        bias and calculates the activation function of the \n",
    "        perceptron.\n",
    "\n",
    "        Args:\n",
    "            X   the data point, should be a numpy\n",
    "                array or a 1xN numpy matrix\n",
    "        Returns:\n",
    "            1   if the activation of X is bigger than 0\n",
    "            0   else\n",
    "        \"\"\"\n",
    "        return 1 if np.append(1, X) @ self.w > 0 else 0\n",
    "\n",
    "    def adaption(self, X, t):\n",
    "        \"\"\"\n",
    "        Trains the perceptron. Adjusts the weights according to \n",
    "        the learning rate and the error between the activation and the \n",
    "        label (delta).\n",
    "\n",
    "        Args:\n",
    "            X   the data point, should be a numpy\n",
    "                array or a 1xN numpy matrix\n",
    "            t   the label\n",
    "        \"\"\"\n",
    "        self.w += self.epsilon * (t - self.activation(X)) * np.append(1, X)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, len(self.w) - 1, self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell instantiates a BasePerceptron (`input_dim` is taken from above, where the data is created) and trains and tests it. The multilayer perceptron will behave exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a perceptron.\n",
    "epsilon = 0.03\n",
    "perceptron = BasePerceptron(input_dim, epsilon)\n",
    "\n",
    "# Train and test the perceptron for data D.\n",
    "train_perceptron(perceptron, D, T, epochs=100, sample_size=20)\n",
    "print()\n",
    "_ = test_perceptron(perceptron, D, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a perceptron.\n",
    "epsilon = 0.03\n",
    "perceptron = BasePerceptron(2, epsilon)\n",
    "\n",
    "# Train and test the perceptron for data D2.\n",
    "train_perceptron(perceptron, D2, T2, epochs=100, sample_size=20)\n",
    "print()\n",
    "_ = test_perceptron(perceptron, D2, T2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the MLP you need two basic building blocks: Input neurons and neurons for the hidden layers. Below there are two unfinished class definitions which inherit from the `BasePerceptron`. That means, that they have the exact same methods already available! This is cool, as you don't need to implement your own initialization or `__repr__` functions.\n",
    "\n",
    "Note however, that some methods are *redefined*. We adjusted the documentation for you, but we just copied the original code. \n",
    "\n",
    "One big difference between the `BasePerceptron` and the `InputPerceptron` and `ContinuousPerceptron` is, that the `BasePerceptron` is used by the functions provided above, while the `InputPerceptron` and `ContinuousPerceptron` are used by the `MultilayerPerceptron` which comes next.\n",
    "\n",
    "To implement these classes it might be needed that you get a better understanding of how they are used: If you don't figure out directly how they should work, continue with the `MultilayerPerceptron` below and come back whenever you need to.\n",
    "\n",
    "Below the `MultilayerPerceptron` there is a cell to test your implementation. You might want to change the number of neurons per (hidden) layer to see how your MLP handles it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The algorithm again**\n",
    "This is just a copy of the algorithm above so that you have it close by while implementing the next few classes.\n",
    "\n",
    "1. **Initialize your MLP.** Use as many input neurons as there are dimensions in the data. Input neurons always expect 1D input. Then create neurons for each hidden and the output layer. Each neuron in the hidden and output layers expects as many inputs as there are neurons in the layer before them.\n",
    "1. **Initialize the neurons' weights.** For each neuron in layers $1 \\dots L_H + 1$ initialize the weights to small random values (values between $0$ and $1$ are fine, but you are allowed to tweak the numbers around).\n",
    "1. **Implement the activation (feed-forward) step.**\n",
    "    1. Decompose the input into its components and pass them to the correct input neuron.\n",
    "    1. Each input neuron passes its unprocessed input to the next layer. That means each neuron in layer $1$ receives all outputs from each input layer as its own input.\n",
    "    $$o_i(0) = x_i$$\n",
    "    1. Calculate the weighted sums of their inputs and apply their activation function $\\sigma$ for each neuron in the layers $1 \\dots L_H + 1$. This is best done iteratively layer by layer, as each layer's input is the output of its preceding layer (Note: $w_{j0}(k,k)$ denotes the bias for neuron $j$ in layer $k$):\n",
    "    $$\\begin{align*}\n",
    "      o_j(k) = \\sigma\\left(w_{j0}(k,k)+\\sum\\limits_{i=1}^{N(k-1)} \n",
    "              o_i(k-1) w_{ji}(k,k-1)\\right)\n",
    "    \\end{align*}$$\n",
    "    with \n",
    "    $$\\sigma(x) = \\frac{1}{1 + \\exp{(-x)}}$$\n",
    "    1. The resulting $o_i(L_H+1)$ are the outputs $y_i$ for each output neuron $i$.\n",
    "1. **Implement the adaption (backpropagation) step.**\n",
    "    1. Compute the error between the target and output components to calculate the error signals $\\delta_i(L_H+1)$ by multiplying them with the derivative $\\sigma^\\prime(x)$:\n",
    "    $$\\begin{align*}\n",
    "      \\delta_i(L_H + 1) &= \\sigma^\\prime(o_i(L_H + 1))\\ (t_i - o_i(L_H + 1)) \\\\\n",
    "                        &= o_i(L_H + 1)\\ (1 - o_i(L_H + 1))\\ (t_i - o_i(L_H + 1))\n",
    "    \\end{align*}$$\n",
    "    1. Calculate the the error signals $\\delta_i(k)$ for each hidden layer $k$, starting with $k=L_H$ and going down to $k=1$.\n",
    "    $$\\begin{align*}\n",
    "    \\delta_i(k) &= \\sigma^\\prime(o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1) \\\\\n",
    "                &= o_i(k)\\ (1 - o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)\n",
    "    \\end{align*}$$\n",
    "    1. Adapt the weights for each neuron in the hidden and output layers.\n",
    "    $$\\Delta w_{ji}(k+1, k) = \\epsilon \\delta_j(k+1) o_i(k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InputPerceptron(BasePerceptron):\n",
    "    \"\"\"\n",
    "    InputPerceptron inherits all properties of the \n",
    "    BasePerceptron but implements a new activation function:\n",
    "    Instead of just using a threshold, it ignores its \n",
    "    weights and just passes on its input.\n",
    "    \"\"\"\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        The activation function for input perceptrons is\n",
    "        the identity function.\n",
    "\n",
    "        Args:\n",
    "            X           the data point\n",
    "        Returns:\n",
    "            X\n",
    "        \"\"\"\n",
    "        # TODO: Change this method.\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "class ContinuousPerceptron(BasePerceptron):\n",
    "    \"\"\"\n",
    "    ContinuousPerceptron inherits all properties of the \n",
    "    BasePerceptron but implements a new activation function:\n",
    "    Instead of just using a threshold, the continuous perceptron\n",
    "    uses a sigmoid function.\n",
    "    \"\"\"\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        The activation function. Prepends a 1 to X for the\n",
    "        bias and calculates the activation function of the \n",
    "        perceptron.\n",
    "\n",
    "        Args:\n",
    "            X           the data point, should be a numpy\n",
    "                        array or a 1xN numpy matrix\n",
    "        Returns:\n",
    "            1 / (1 + exp( -y ))\n",
    "            where y is the dot product of the weights and \n",
    "            the padded input.\n",
    "        \"\"\"\n",
    "        # TODO: Change this method.\n",
    "        return scipy.special.expit(self.w @ np.append(1, X))\n",
    "\n",
    "    def adaption(self, X, t):\n",
    "        \"\"\"\n",
    "        Trains the perceptron. Adjusts the weights according to \n",
    "        the learning rate and the error between the activation and the \n",
    "        label (delta).\n",
    "\n",
    "        Args:\n",
    "            X   the data point, should be a numpy\n",
    "                array or a 1xN numpy matrix\n",
    "            t   the delta\n",
    "        \"\"\"\n",
    "        # TODO: Change this method.\n",
    "        self.w += self.epsilon * t * np.append(1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultilayerPerceptron:\n",
    "    def __init__(self, dimensions=[2, 1, 1], epsilon=0.03):\n",
    "        \"\"\"\n",
    "        Initializes a multilayer perceptron.\n",
    "        \n",
    "        The argument dimensions denotes the number of neurons \n",
    "        per layer. The default is [2, 1, 1], which means that\n",
    "        there are 2 InputPerceptrons in the input layer, \n",
    "        1 ContinuousPerceptron in the first hidden layer and \n",
    "        also 1 ContinuousPerceptron in the output layer.\n",
    "        \n",
    "        Epsilon is passed on to all neurons in the MLP (except\n",
    "        for the input layer).\n",
    "        \n",
    "        The layers are stored as one list per layer containing\n",
    "        its neurons. All layers are stored in a list self.layers.\n",
    "        \n",
    "        Example:\n",
    "        self.layers = [[input neurons],\n",
    "                       [neurons of layer 1],\n",
    "                       [neurons of layer 2],\n",
    "                       [neurons of layer 3],\n",
    "                       [output neurons]]\n",
    "        \n",
    "        Args:\n",
    "            dimensions  A list of integers denoting the number \n",
    "                        of neurons per layer. The first integer\n",
    "                        denotes the number of input neurons,\n",
    "                        the last the number of output neurons.\n",
    "                        All integers in between denote neurons\n",
    "                        per hidden layer.\n",
    "            epsilon     The learning rate passed on to each neuron.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Create input layer.\n",
    "        self.layers.append([InputPerceptron(1, 0) for i in range(dimensions[0])])\n",
    "\n",
    "        # Generate hidden and output layers.\n",
    "        dim = len(self.layers[0])\n",
    "        for N in dimensions[1:]:\n",
    "            layer = [ContinuousPerceptron(dim, epsilon) for i in range(N)]\n",
    "            self.layers.append(layer)\n",
    "            dim = N\n",
    "\n",
    "        # Initialize outputs and deltas.\n",
    "        self.outputs = []\n",
    "        self.deltas = []\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        Implements the activation step (feed forward).\n",
    "        \n",
    "        Calculates the outputs for each layer by calling the \n",
    "        respective activation methods and stores the results\n",
    "        in a list of lists self.outputs such that\n",
    "        self.outputs[k][i] is the output of neuron self.layers[k][i]\n",
    "        where k is the layer and i is the i-th neuron of that layer.\n",
    "        \n",
    "        Args:\n",
    "            X   the input data\n",
    "        \n",
    "        Returns:\n",
    "            the network output, i.e. the outputs of the output layer.\n",
    "        \"\"\"\n",
    "        # Clear potentially stored outputs.\n",
    "        self.outputs = []\n",
    "\n",
    "        # Activate input layer and store its outputs.\n",
    "        layer_outputs = np.array([self.layers[0][i].activation(x) for i, x in enumerate(X)])\n",
    "        self.outputs.append(layer_outputs)\n",
    "\n",
    "        # Activate all other layers with the outputs from before.\n",
    "        for layer in self.layers[1:]:\n",
    "            layer_outputs = np.array([layer[j].activation(layer_outputs) for j in range(len(layer))])\n",
    "            self.outputs.append(layer_outputs)\n",
    "\n",
    "        # Return the last outputs for the output layer.\n",
    "        return np.copy(layer_outputs)\n",
    "\n",
    "    def adaption(self, X, t):\n",
    "        \"\"\"\n",
    "        Implements the adaption step (backpropagation).\n",
    "\n",
    "        Calls self.activation(X) to make sure the outputs are set\n",
    "        properly.\n",
    "        \n",
    "        Calculates the error signals for the output layer and \n",
    "        from there on calculates the error signals for each hidden \n",
    "        layer, starting with the last.\n",
    "        \n",
    "        Then adjusts each hidden and output neuron's weights by \n",
    "        calling the respective adaption functions.\n",
    "        \n",
    "        Args:\n",
    "            X   the data point\n",
    "            t   the label (must fit the number of output components)\n",
    "        \"\"\"\n",
    "        # Clear potentially stored deltas.\n",
    "        self.deltas = []\n",
    "\n",
    "        # Activate perceptron to figure out and store each\n",
    "        # neuron's output.\n",
    "        outputs = self.activation(X)\n",
    "\n",
    "        # Compute error:\n",
    "        error = (t - outputs)\n",
    "\n",
    "        # Calculate deltas for output layer and store them.\n",
    "        layer_deltas = outputs * (1 - outputs) * error\n",
    "        self.deltas.insert(0, layer_deltas)\n",
    "\n",
    "        # Calculate other deltas.\n",
    "        for k in range(len(self.layers) - 2, 0, -1):\n",
    "            layer_deltas = []\n",
    "            for i, neuron in enumerate(self.layers[k]):\n",
    "                derivative = self.outputs[k][i] * (1 - self.outputs[k][i])\n",
    "                weights = np.array([j.w[i] for j in self.layers[k + 1]])\n",
    "                layer_deltas.append(derivative * weights @ self.deltas[0])\n",
    "            self.deltas.insert(0, np.array(layer_deltas))\n",
    "\n",
    "        # Adapt weights for hidden and output neurons.\n",
    "        for k, layer in enumerate(self.layers[1:]):\n",
    "            for i, neuron in enumerate(layer):\n",
    "                # Both are accessed via k (instead of k and k + 1)\n",
    "                # as there are no deltas for the input layer \n",
    "                # and we skip that - so outputs[0] is \n",
    "                # 0's input while deltas[0] is 1's delta!\n",
    "                neuron.adaption(self.outputs[k], self.deltas[k][i])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'MultiLayerPerceptron({}, {})'.format([len(l) for l in self.layers], self.epsilon)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n\\t'.join([repr(self)] + [str(l) for l in self.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a multilayer perceptron.\n",
    "epsilon = 0.03\n",
    "output_dim = 1\n",
    "layers = [input_dim, 2, 2, output_dim]\n",
    "perceptron = MultilayerPerceptron(layers, epsilon)\n",
    "\n",
    "# Train and test the perceptron for data D.\n",
    "train_perceptron(perceptron, D, T, epochs=100, sample_size=20)\n",
    "print()\n",
    "_ = test_perceptron(perceptron, D, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a perceptron.\n",
    "epsilon = 0.03\n",
    "output_dim = 1\n",
    "layers = [2, 5, 5, output_dim]\n",
    "perceptron = MultilayerPerceptron(layers, epsilon)\n",
    "\n",
    "# Train and test the perceptron for data D2.\n",
    "train_perceptron(perceptron, D2, T2, epochs=1000, sample_size=20)\n",
    "print()\n",
    "_ = test_perceptron(perceptron, D2, T2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: MLP and RBFN [10 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
