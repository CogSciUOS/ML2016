{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 12, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Multilayer Perceptron (MLP) [10 Points]\n",
    "\n",
    "Last week you implemented a simple perceptron. This week we already provide some basic perceptron which you will adjust to build network from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SimplePerceptron:\n",
    "    \"\"\"\n",
    "    A simple perceptron implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions=100, epsilon=0.03):\n",
    "        \"\"\"\n",
    "        Initializes the perceptron. Creates dimensions + 1\n",
    "        random weights (the additional weight is the bias.)\n",
    "\n",
    "        Args:\n",
    "            dimensions  the data dimensionality N\n",
    "            epsilon     the learning rate\n",
    "        \"\"\"\n",
    "        self.w = np.random.rand(dimensions + 1)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        The activation function. Prepends a 1 to X for the\n",
    "        bias and calculates the activation function of the \n",
    "        perceptron.\n",
    "\n",
    "        Args:\n",
    "            X           the data point, should be a numpy\n",
    "                        arary or a 1xN numpy matrix\n",
    "\n",
    "        Returns:\n",
    "            True  if the activation of X is bigger than 0\n",
    "            False elseal\n",
    "        \"\"\"\n",
    "        return np.append(1, X) @ self.w > 0\n",
    "\n",
    "    def train(self, X, t):\n",
    "        \"\"\"\n",
    "        Trains the perceptron. Adjusts the weights according to \n",
    "        the learning rate and the error between the activation and t.\n",
    "\n",
    "        Args:\n",
    "            X           the data point, should be a numpy\n",
    "                        arary or a 1xN numpy matrix\n",
    "            t           the label for this data point should be\n",
    "                        True or False\n",
    "        \"\"\"\n",
    "        self.w += self.epsilon * (t - self.activation(X)) * np.append(1, X)\n",
    "\n",
    "# Generate some data.\n",
    "N = 1000\n",
    "dim = 3\n",
    "D = np.random.rand(1000, dim)\n",
    "# Label data: sum should be > 0.8 * dim\n",
    "D = np.hstack((D, np.matrix(np.sum(D, 1) > 0.8 * dim).T))\n",
    "\n",
    "# Instantiate a Perceptron.\n",
    "perceptron = SimplePerceptron(D.shape[1] - 1)\n",
    "\n",
    "# Train the perceptron for several epochs.\n",
    "epochs = 20\n",
    "sample_size = 100\n",
    "for epoch in range(epochs):\n",
    "    for sample in range(sample_size):\n",
    "        sample_data = D[np.random.choice(range(N), replace=False),:]\n",
    "        for data in sample_data:\n",
    "            x = data[0,0:-1]\n",
    "            t = data[0,-1]\n",
    "            perceptron.train(x, t)\n",
    "\n",
    "# Test the perceptron on all data.\n",
    "error = 0\n",
    "for data in D:\n",
    "    error += np.abs(data[0,-1] - perceptron.activation(data[0,0:-1])) / N\n",
    "print(\"The perceptron classifies {:.2%} of the data correctly.\".format(1 - error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, dimensions=100, epsilon=0.03):\n",
    "        self.w = np.random.rand(dimensions + 1)\n",
    "        self.epsilon = epsilon\n",
    "        self.y = None\n",
    "\n",
    "    def activation(self, X):\n",
    "        self.y = ...\n",
    "        return self.y\n",
    "\n",
    "    def train(self, X, t, delta):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, neurons_per_layer):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: MLP and RBFN [10 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This exercise is aimed at deepening the understanding of Radial Basis Function Networks and how they relate to Multilayer Perceptrons. Not all of the answers can be found directly in the slides - so when answering the (more algorithmic) questions, first take a minute and think about how you would go about solving them and if nothing comes to mind search the internet for a little bit. If you are interested in a real life application of both algorithms and how they compare take a look at this paper: [Comparison between Multi-Layer Perceptron and Radial Basis Function Networks for Sediment Load Estimation in a Tropical Watershed](http://file.scirp.org/pdf/JWARP20121000014_80441700.pdf)\n",
    "\n",
    "![Schematic of a RBFN](RBFN.png)\n",
    "\n",
    "We have prepared a little example that shows how radial basis function approximation works in Python. This is not an example implementation of a RBFN but illustrates the work of the hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def func(x,y):\n",
    "    '''\n",
    "    This is the example function that should be fitted.\n",
    "    Its shape could be described as two peaks close to\n",
    "    each other - one going up, the other going down\n",
    "    '''\n",
    "    return (x + y) * np.exp(-4.0 * (x**2 + y**2))\n",
    " \n",
    "x = uniform(-1.0, 1.0, size=50)\n",
    "y = uniform(-1.0, 1.0, size=50)\n",
    "\n",
    "# sample 50 random datapoints from the underlying function\n",
    "fvals = func(x, y)\n",
    "\n",
    "# get the aprroximation via RBF\n",
    "new_func = Rbf(x, y, fvals)\n",
    "\n",
    "# sample 100x100 values from the approximated function\n",
    "x_new, y_new = np.mgrid[-1:1:100j, -1:1:100j]\n",
    "f_new = new_func(x_new, y_new)\n",
    "\n",
    "plt.figure(\"Original Function\")\n",
    "# This plot represents the original function\n",
    "x_orig, y_orig = np.mgrid[-1:1:100j, -1:1:100j]\n",
    "data_orig = func(x_orig, y_orig)\n",
    "plt.imshow(data_orig, extent=[-1,1,-1,1], cmap=plt.cm.jet)\n",
    "\n",
    "plt.figure(\"RBF Result\")\n",
    "# This plots the approximation of the original function by the RBF\n",
    "# if the plot looks strange try to run it again, the sampling\n",
    "# in the beginning is random\n",
    "plt.imshow(f_new, extent=[-1,1,-1,1], cmap=plt.cm.jet)\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "# scatter the datapoints that have been used by the RBF\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are radial basis functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial basis functions are all functions that fullfill the following criteria:\n",
    "\n",
    "The value of the function for a certain point depends only on the distance of that point to the origin or some other fixed center point. In mathematical formulation that spells out to: \n",
    "$\\phi (\\mathbf {x} )=\\phi (\\|\\mathbf {x} \\|)$  or  $\\phi (\\mathbf {x} ,\\mathbf {c} )=\\phi (\\|\\mathbf {x} -\\mathbf {c} \\|)$. Notice that it is not necessary (but most common) to use the norm as the measure of distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the structure of a RBFN? You may also use the notion from the above included picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBFN's are networks that contain only one hidden layer. The input is connected to all the hidden units. Each of the hidden units has a different radial basis function that is *sensitive* to ranges in the input domain. The output is then a linear combination of the outpus ot those functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is a RBFN trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all input data has to be normalized.\n",
    "\n",
    "Training a RBFN is a two-step process. First the functions in the hidden layer are initialized. This can be either done by sampling from the input data or by first performing a k-means clustering, where k is the number of nodes that have to be initialzed.\n",
    "\n",
    "The second step fits a linear model with coefficients $w_{i}$ to the hidden layer's outputs with respect to some objective function. The objective function depends on the task: it can be the least squares function, or the weights can be adapted by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to the Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do both models have in common? Where do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|RBFN                 |MLP                  | \n",
    "|---------------------|---------------------|\n",
    "| non-linear layered feedforward network|non-linear layered feedforward network| \n",
    "| hidden neurons use radial basis functions, output neurons use linear function| input, hidden and output-layer all use the same activation function| \n",
    "| universal approximator |   universal approximator |\n",
    "| learning usually affects only one or some RBF | learning affects many weights throught the network|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can classification in both networks be visualized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Classification](Solution_Classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When would you use a RBFN instead of a Multilayer Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBFNs are more robust to noise and should therefore be used when the data contains false-positives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
