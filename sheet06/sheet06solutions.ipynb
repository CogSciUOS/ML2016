{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 29, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Hebbian Learning (6 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture (ML-07, Slides 10ff.) there is a simplified version of Ivan Pavlov's famous experiment on classical conditioning. In this exercise you will take a look into this simplified model and create your own conditionable dog with a simple Hebbian learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### a) Programming a Dog\n",
    "To model the dog saliva behavior we will need to model an unconditioned and a conditioned stimulus: food and bell. They are represented as lists: `weight_food` and `weight_bell`. Note that one could just use a single number, the lists are only here to keep track of the history for a nice output. It is possible to access the current weight by selecting the last item of each list, respectively: `weight_food[-1]`.\n",
    "\n",
    "A list of trials is already given as well as a condition database. Each entry represents an index to select from the `condition_db`. To figure out the value of the stimulus `food` in the second trial (which maps to condition `1`) one could do: `condition_db[1][\"food\"]`.\n",
    "\n",
    "Your task is to implement a `for` loop over all trials. In each iteration select the correct values for $x_1$ and $x_2$ from the condition database and retrieve the current weights $w_1$ and $w_2$. Then calculate the response of the dog with the threshold $\\theta$:\n",
    "\n",
    "$$\n",
    "r_t = \\Theta(x_{1,t-1} w_{1,t-1} + x_{2,t-1} w_{2,t-1})\\\\\n",
    "\\Theta(x)= \\begin{cases}1 \\text{ if } x >= \\theta\\\\0 \\text{ else }\\end{cases}\n",
    "$$\n",
    "\n",
    "With this response calculate both $w_{n,t}$ according to the Hebbian rule:\n",
    "\n",
    "$$w_{n,t} = w_{n, t-1} + \\epsilon \\cdot r_t \\cdot x_{n,t}$$\n",
    "\n",
    "*Note: While you program the output might look a little messy, don't worry about it. Once you fill up all three lists properly, it will look much like on ML-07, Slide 14.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "condition_db = [{\"food\": 1, \"bell\": 0}, \n",
    "                {\"food\": 0, \"bell\": 1},\n",
    "                {\"food\": 1, \"bell\": 1}]\n",
    "\n",
    "trials = [0, 1, 2, 2, 1, 2, 1]\n",
    "\n",
    "epsilon = 0.2\n",
    "theta = 1/2\n",
    "\n",
    "responses = []\n",
    "weight_food = [1]\n",
    "weight_bell = [0]\n",
    "\n",
    "# TODO: For each trial, update the current weights of the US and CS and store\n",
    "# the results in the respective lists. Also store the response.\n",
    "\n",
    "for trial in trials:\n",
    "    x_1 = condition_db[trial][\"food\"]\n",
    "    x_2 = condition_db[trial][\"bell\"]\n",
    "    w_1 = weight_food[-1]\n",
    "    w_2 = weight_bell[-1]\n",
    "    \n",
    "    response = x_1 * w_1 + x_2 * w_2 >= theta\n",
    "    responses.append(response)\n",
    "    \n",
    "    weight_food.append(w_1 + epsilon * response * x_1)\n",
    "    weight_bell.append(w_2 + epsilon * response * x_2)\n",
    "\n",
    "# Output\n",
    "print(\"| Food   |   |\" + \"|   |\".join([\"{:3d}\".format(condition_db[trial][\"food\"]) for trial in trials]) + \"|   |\")\n",
    "print(\"| Bell   |   |\" + \"|   |\".join([\"{:3d}\".format(condition_db[trial][\"bell\"]) for trial in trials]) + \"|   |\")\n",
    "print(\"| Saliva |   |\" + \"|   |\".join([\"{:3d}\".format(response) for response in responses]) + \"|   |\")\n",
    "print(\"| w_Food |\" + \"|   |\".join([\"{:3.1f}\".format(w) for w in weight_food]) + \"|\")\n",
    "print(\"| w_Bell |\" + \"|   |\".join([\"{:3.1f}\".format(w) for w in weight_bell]) + \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Parameter adjustment\n",
    "\n",
    "In the above default setting of trials (`[0, 1, 2, 2, 1, 2, 1]`, in case you changed it), how many learning steps did you need until the dog started to produce saliva on the conditioned stimulus? What happens if you change the parameters $\\epsilon$ and $\\theta$? Try smaller and bigger values for each or present different conditions to the dog."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Three learning steps were needed to train the dog to react to the conditioned stimulus.\n",
    "\n",
    "With a smaller epsilon the number of steps increases, while with a smaller threshold the number of steps decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Local PCA (8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the lecture we learned that regular PCA is ill suited for special cases of data. In this assignment we will take a look at local PCA which is used for clustered data (ML-06, Slide 25). This is mostly a repetition of algorithms we already used. Feel free to use the built-in functions for k-means clustering and PCA from the libraries (we already included the right imports to set you on track)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import multivariate_normal as multNorm\n",
    "\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "# Generate clustered data - you may plot the data to take a look at it\n",
    "data = np.vstack((multNorm([2,2],[[0.1, 0], [0, 1]],100), multNorm([-2,-4],[[1, 0], [0, 0.3]],100)))\n",
    "# Look at the data. (Will be plotted below as well.)\n",
    "# plt.figure('Data')\n",
    "# plt.scatter(*zip(*data))\n",
    "\n",
    "# TODO: Apply k-means to the data.\n",
    "\n",
    "# Compute k-means with K = 2 (2 clusters).\n",
    "centroids, _ = kmeans(data,2)\n",
    "\n",
    "# Assign each sample to a cluster.\n",
    "idx, _ = vq(data, centroids)\n",
    "\n",
    "# TODO: Apply PCA for each cluster and store each two largest components.\n",
    "pcs = np.empty([np.unique(idx).size, 2, 2])\n",
    "for idxU in np.unique(idx):\n",
    "    pca = PCA(2)\n",
    "    pca.fit(data[idx==idxU])\n",
    "    pcs[idxU] = pca.components_\n",
    "\n",
    "# TODO: Plot the results of k-means and local PCA\n",
    "fig = plt.figure('Local PCA')\n",
    "plt.gca().axis('equal')\n",
    "\n",
    "# make cluster centroids the same format as the PCs\n",
    "cls = np.tile(centroids[np.newaxis], (pcs.shape[0], 1, 1))\n",
    "\n",
    "# plot data, PCs and centroids\n",
    "for i, idxU in enumerate(np.unique(idx)):\n",
    "    plt.plot(*zip(*data[idx==idxU]), 'o', alpha=0.4)\n",
    "    plt.quiver(*cls[:,i,:].T, *pcs[i,:,:].T, scale_units='xy', scale=0.7)\n",
    "\n",
    "_ = plt.plot(centroids[:,0], centroids[:,1], '*y', markeredgewidth=0.5, markersize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Data Visualization and Chernoff Faces (6 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following exercise contains no programming (unless you want to go through the implementation). Answer the questions that are posted below the code segment (and run the code before - it's really worth it!). In case you are even more interested - here is a link to the [original paper](http://www.dtic.mil/cgi-bin/GetTRDoc?AD=AD0738473)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from pylab import *\n",
    "\n",
    "def cface(ax, x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18):\n",
    "    '''\n",
    "    This implementation of chernov faces is taken from Abraham Flaxman. You can\n",
    "    find the original source files here: https://gist.github.com/aflaxman/4043086\n",
    "    Only minor adjustments have been made.\n",
    "    \n",
    "     x1 = height  of upper face\n",
    "     x2 = overlap of lower face\n",
    "     x3 = half of vertical size of face\n",
    "     x4 = width of upper face\n",
    "     x5 = width of lower face\n",
    "     x6 = length of nose\n",
    "     x7 = vertical position of mouth\n",
    "     x8 = curvature of mouth\n",
    "     x9 = width of mouth\n",
    "     x10 = vertical position of eyes\n",
    "     x11 = separation of eyes\n",
    "     x12 = slant of eyes\n",
    "     x13 = eccentricity of eyes\n",
    "     x14 = size of eyes\n",
    "     x15 = position of pupils\n",
    "     x16 = vertical position of eyebrows\n",
    "     x17 = slant of eyebrows\n",
    "     x18 = size of eyebrows\n",
    "    '''\n",
    "    \n",
    "    # transform some values so that input between 0,1 yields variety of output\n",
    "    x3 = 1.9*(x3-.5)\n",
    "    x4 = (x4+.25)\n",
    "    x5 = (x5+.2)\n",
    "    x6 = .3*(x6+.01)\n",
    "    x8 = 5*(x8+.001)\n",
    "    x11 /= 5\n",
    "    x12 = 2*(x12-.5)\n",
    "    x13 += .05\n",
    "    x14 += .1\n",
    "    x15 = .5*(x15-.5)\n",
    "    x16 = .25*x16\n",
    "    x17 = .5*(x17-.5)\n",
    "    x18 = .5*(x18+.1)\n",
    "\n",
    "    # top of face, in box with l=-x4, r=x4, t=x1, b=x3\n",
    "    e = mpl.patches.Ellipse( (0,(x1+x3)/2), 2*x4, (x1-x3), fc='white', linewidth=2)\n",
    "    ax.add_artist(e)\n",
    "\n",
    "    # bottom of face, in box with l=-x5, r=x5, b=-x1, t=x2+x3\n",
    "    e = mpl.patches.Ellipse( (0,(-x1+x2+x3)/2), 2*x5, (x1+x2+x3), fc='white', linewidth=2)\n",
    "    ax.add_artist(e)\n",
    "\n",
    "    # cover overlaps\n",
    "    e = mpl.patches.Ellipse( (0,(x1+x3)/2), 2*x4, (x1-x3), fc='white', ec='none')\n",
    "    ax.add_artist(e)\n",
    "    e = mpl.patches.Ellipse( (0,(-x1+x2+x3)/2), 2*x5, (x1+x2+x3), fc='white', ec='none')\n",
    "    ax.add_artist(e)\n",
    "    \n",
    "    # draw nose\n",
    "    plot([0,0], [-x6/2, x6/2], 'k')\n",
    "    \n",
    "    # draw mouth\n",
    "    p = mpl.patches.Arc( (0,-x7+.5/x8), 1/x8, 1/x8, theta1=270-180/pi*arctan(x8*x9), theta2=270+180/pi*arctan(x8*x9))\n",
    "    ax.add_artist(p)\n",
    "    \n",
    "    # draw eyes\n",
    "    p = mpl.patches.Ellipse( (-x11-x14/2,x10), x14, x13*x14, angle=-180/pi*x12, facecolor='white')\n",
    "    ax.add_artist(p)\n",
    "    \n",
    "    p = mpl.patches.Ellipse( (x11+x14/2,x10), x14, x13*x14, angle=180/pi*x12, facecolor='white')\n",
    "    ax.add_artist(p)\n",
    "\n",
    "    # draw pupils\n",
    "    p = mpl.patches.Ellipse( (-x11-x14/2-x15*x14/2, x10), .05, .05, facecolor='black')\n",
    "    ax.add_artist(p)\n",
    "    p = mpl.patches.Ellipse( (x11+x14/2-x15*x14/2, x10), .05, .05, facecolor='black')\n",
    "    ax.add_artist(p)\n",
    "    \n",
    "    # draw eyebrows\n",
    "    plot([-x11-x14/2-x14*x18/2,-x11-x14/2+x14*x18/2],[x10+x13*x14*(x16+x17),x10+x13*x14*(x16-x17)],'k')\n",
    "    plot([x11+x14/2+x14*x18/2,x11+x14/2-x14*x18/2],[x10+x13*x14*(x16+x17),x10+x13*x14*(x16-x17)],'k')\n",
    "\n",
    "fig = figure('Chernov Faces', figsize=(11,11))\n",
    "for i in range(25):\n",
    "    ax = fig.add_subplot(5, 5, i+1, aspect='equal')\n",
    "    cface(ax, .9, *rand(17))\n",
    "    ax.axis([-1.2,1.2,-1.2,1.2])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Data Visualization Techniques\n",
    "\n",
    "Why do we need data visualization techniques and what are techniques to visualize high dimensional data?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sometimes it is necessary to visualize high dimensional data and a projection via PCA or similar methods might not help enough: We might lose information in a 2D projection.\n",
    "\n",
    "In those cases it is useful to come up with other representations of data which we could potentially print on a sheet of paper.\n",
    "\n",
    "Techniques are usually glyphs, but different kinds of projection might already be enough (taking information loss into account)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Chernoff faces\n",
    "\n",
    "Why did Chernoff use faces for his representation? Why not something else, like dogs or houses?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Humans are exceptionally good at face recognition. It is very easy to realize if one eye is bigger than another or eye brows are closer together in face-like images for humans than for example figuring out differences in windows sizes or changes in roof skewness between houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Alternatives\n",
    "\n",
    "Explain at least one other data visualization technique from the lecture."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Scatterplot matrix: Scatter data into plots for each combination of two attributes.\n",
    "- Glyphs: Map data dimensions onto parameters for geometrical figures, e.g. star glyphs, arrows. Properties could be lengths, widths, orientations, colors, ...\n",
    "- Parallel coordinates: Use feature dimensions as one axis and feature values as another. Plot datapoints as lines.\n",
    "- Projections: Several different techniques to project the data: PCA, scaling strategies, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
