{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 03: Basics of Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 1, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder.\n",
    "\n",
    "There are a lot of implementations with fewer theory questions on this sheet, but don't worry: To be able to implement most of the code, you have to understand the theory.\n",
    "\n",
    "This week's assignments make use of two packages: `numpy` and `matplotlib`. We already expected you to install those as part of sheet 1. If you did not do so, go back to those instructions or just run the following command in the `terminal`/`cmd.exe` to do so (Mac/Linux: Might require `sudo`; Windows: Use `pip` instead of `pip3`). This will also upgrade your current installation.\n",
    "\n",
    "    pip3 install --upgrade jupyter numpy matplotlib\n",
    "\n",
    "One note about `matplotlib`: If you run code which contains a plot like the cell below, it can sometimes take a while to execute the code and show the results. During that process the invocation count will be shown as a little Asterisk (\\*) like this:\n",
    "\n",
    "    In [*]:\n",
    "\n",
    "Just be patient for a few seconds. The following cell tests if `numpy` and `matplotlib` are installed and work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert importlib.util.find_spec('numpy') is not None , 'numpy not found'\n",
    "assert importlib.util.find_spec('matplotlib') is not None, 'matplotlib not found'\n",
    "\n",
    "figure_intro = plt.figure('Example plot')\n",
    "plt.plot(np.random.randn(1000, 1))\n",
    "figure_intro.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Rosner test [5 Points]\n",
    "\n",
    "The Rosner test is an iterative procedure to remove outliers of a data set via a z-test. In this exercise you will implement it and apply it to a sample data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Outliers\n",
    "\n",
    "First of all, think about why we use procedures like this and answer the following questions: \n",
    "\n",
    "What are causes for outliers? And what are our options to deal with them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of outliers which can have different causes. They could arise through measurement or technical errors when collecting data. This may be connected to having a sharp cut-off in regard to the range of measurements, which could lead to a high concentration of values at the artificial boundaries of an experiment. However they may also show us a true underlying effect in our data that we didn't expect or account for. This might be the case when we are treating the measurements as a single distribution, when in reality there are actually two underlying distributions. Lastly, our distribution might actually naturally have a high variance, which makes outliers or extreme values a natural part of the distribution.\n",
    "\n",
    "First, we need to detect probable outliers. In order to decide which data points we want to declare as an outlier we have to find a model for regular, meaning \"not outlying\", data points. What we do most of the time is to assume a normal distribution underlying the data (or a multivariate distribution where each cluster is normally distributed).\n",
    "\n",
    "One option is to calculate the z-value for each data point (a measure of the distance from the mean in terms of the standard deviation) -- data points with a high z-value would be regarded outliers, a common threshold would be a z value bigger than 3. This can be improved by using the median instead of the mean and tweaking the threshold. The Rosner test takes it one step further by iteratively calculating z-values and removing found outliers until none can be found anymore. This can be done one outlier at a time or k outliers at a time for more efficiency. \n",
    "\n",
    "A different approach would be to not remove the outliers completely, but to weight them according to the z-values. And lastly an alternative for complete removal would be to fill up the emerging gaps with values that fit the distribution better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Rosner test\n",
    "\n",
    "In the following you find a stub for the implementation. The dataset is already generated. Now it is your turn to write the Rosner test and detect the outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate dataset\n",
    "data = np.random.normal(50, 20, 100)\n",
    "xtr_points = np.random.normal(-50, 10, 6)\n",
    "\n",
    "data = np.concatenate((xtr_points, data))\n",
    "outliers = []\n",
    "\n",
    "# just to check if everything is pretty\n",
    "fig_rosner_data = plt.figure('The Dataset')\n",
    "plt.plot(data, 'x')\n",
    "fig_rosner_data.canvas.draw()\n",
    "\n",
    "# TODO: now find the outliers!\n",
    "# Add them to 'outliers' and remove them from 'data'.\n",
    "z = float('inf')\n",
    "\n",
    "while z > 3:\n",
    "    stdev = np.std(data)\n",
    "    m = np.mean(data)\n",
    "    zs = [abs(value - m) / stdev for value in data]\n",
    "\n",
    "    z = max(zs)\n",
    "    z_index = zs.index(z)\n",
    "    \n",
    "    # check if we have to remove the value\n",
    "    if z > 3: \n",
    "        outliers.append([z_index, data[z_index]])\n",
    "        data = np.delete(data, z_index)\n",
    "\n",
    "# plot results        \n",
    "fig_rosner = plt.figure('Rosner Result')\n",
    "plt.plot(data,'bx', label='cleared data')\n",
    "plt.scatter([x[0] for x in outliers], [y[1] for y in outliers], c='red', marker='x', label='outliers')\n",
    "plt.legend(loc='lower right');\n",
    "fig_rosner.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: p-norm [5 Points]\n",
    "\n",
    "A very well known norm is the euclidean distance. However, it is not the only norm: It is in fact just one of many p-norms where $p = 2$. In this assignment you will take a look at other p-norms and see how they behave.\n",
    "\n",
    "Implement a function `pdist` which expects a vector $x \\in \\mathcal{R}^n$ and a scalar $p \\geq 1, p \\in \\mathcal{R}$ and returns the p-norm of $x$ which is defined as:\n",
    "\n",
    "$$||x||_p = \\left(\\sum\\limits_{i=1}^n |x_i|^p \\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "*Note:* Even though the norm is only defined for $p \\geq 1$, values $0 < p < 1$ are still interesting. In that case we can not talk about a norm anymore, as the triangle inequality ($||a|| + ||b|| \\geq ||a + b||$) does not hold. We will still take a look at some of these values, so your function should handle them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pdist(x, p):\n",
    "    \"\"\"\n",
    "    Calculates the p-norm of x.\n",
    "    Also allows values between 0 and 1 for p.\n",
    "    \"\"\"\n",
    "    if p <= 0:\n",
    "        raise ValueError('p has to be > 0!')\n",
    "    return np.sum(np.abs(np.array(x)) ** p) ** (1 / p)\n",
    "\n",
    "\n",
    "# 1e-10 is 0.0000000001\n",
    "assert pdist(1, 2)      - 1          < 1e-10 , \"pdist is incorrect for x = 1, p = 2\"\n",
    "assert pdist(2, 2)      - 2          < 1e-10 , \"pdist is incorrect for x = 2, p = 2\"\n",
    "assert pdist([2, 1], 2) - np.sqrt(5) < 1e-10 , \"pdist is incorrect for x = [2, 1], p = 2\" \n",
    "assert pdist(2, 0.5)    - 2          < 1e-10 , \"pdist is incorrect for x = 2, p = 0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement another function `pdist2` which expects two vectors $x_0 \\in \\mathcal{R}^n, x_1 \\in \\mathcal{R}^n$ and a scalar $p \\geq 1, p \\in \\mathcal{R}$ and returns the distance between $x_0$ and $x_1$ on the p-norm defined by $p$. Again handle $0 < p < 1$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pdist2(x0, x1, p):\n",
    "    \"\"\"\n",
    "    Calculates the distance between x0 and x1\n",
    "    given the p-norm with p.\n",
    "    Also allows values between 0 and 1 for p.\n",
    "    \"\"\"\n",
    "    if p <= 0:\n",
    "         ValueError('p has to be > 0!')\n",
    "    return np.sum(np.abs(np.array(x0) - np.array(x1)) ** p) ** (1 / p)\n",
    "\n",
    "\n",
    "# 1e-10 is 0.0000000001\n",
    "assert pdist2(1, 2, 2)           - 1          < 1e-10 , \"pdist2 is incorrect for x0 = 1, x1 = 2, p = 2\"\n",
    "assert pdist2(2, 5, 2)           - 3          < 1e-10 , \"pdist2 is incorrect for x0 = 2, x1 = 5, p = 2\"\n",
    "assert pdist2([2, 1], [1, 2], 2) - np.sqrt(2) < 1e-10 , \"pdist2 is incorrect for x0 = [2, 1], x1 = [1, 2], p = 2\" \n",
    "assert pdist2([2, 1], [0, 0], 2) - np.sqrt(5) < 1e-10 , \"pdist2 is incorrect for x0 = [2, 1], x1 = [0, 0], p = 2\" \n",
    "assert pdist2(2, 0, 0.5)         - 2          < 1e-10 , \"pdist2 is incorrect for x0 = 2, x1 = 0, p = 0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compare some different p-norms. Below is part of a code to plot data in nice scatter plots. \n",
    "\n",
    "Your task is to calculate the data to plot. The variable `data` is currently simply filled with zeros. Instead, fill it as follows:\n",
    "\n",
    "- Use the function `np.linspace()` to create a vector of `50` evenly distributed values between `-100` and `100` (inclusively).\n",
    "- Fill `data`: It should have 2500 rows. Each of the 2500 rows should contain `[x, y, d]`, where `x` is the x coordinate and `y` the y coordinate of a point, and `d` the p-norm of `(x, y)`. Use either `pdist` or `pdist2` to calculate `d`. \n",
    "- Normalize the data in `data[:,2]` (i.e. all d-values) so that they are between 0 and 1.\n",
    "\n",
    "Run your code and take a look at your results. Darker colors mean that a value is closer to the center (0, 0) according to the p-norm used.\n",
    "\n",
    "*Hint:* To give you an idea of how `data` should look like, here is an example for three evenly distributed values between `-1` and `1` and a p-norm with `p = 2`.\n",
    "\n",
    "Before normalization of the d-column:\n",
    "\n",
    "```python\n",
    "data = np.array([[-1.         -1.          1.41421356]\n",
    "                 [-1.          0.          1.        ]\n",
    "                 [-1.          1.          1.41421356]\n",
    "                 [ 0.         -1.          1.        ]\n",
    "                 [ 0.          0.          0.        ]\n",
    "                 [ 0.          1.          1.        ]\n",
    "                 [ 1.         -1.          1.41421356]\n",
    "                 [ 1.          0.          1.        ]\n",
    "                 [ 1.          1.          1.41421356]])\n",
    "```\n",
    "\n",
    "After normalization of the d-column:\n",
    "\n",
    "```python\n",
    "data = np.array([[-1.         -1.          1.        ]\n",
    "                 [-1.          0.          0.70710678]\n",
    "                 [-1.          1.          1.        ]\n",
    "                 [ 0.         -1.          0.70710678]\n",
    "                 [ 0.          0.          0.        ]\n",
    "                 [ 0.          1.          0.70710678]\n",
    "                 [ 1.         -1.          1.        ]\n",
    "                 [ 1.          0.          0.70710678]\n",
    "                 [ 1.          1.          1.        ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "color = ColorConverter()\n",
    "figure_norms = plt.figure('p-norm comparison')\n",
    "\n",
    "# TODO: create the linspace vector\n",
    "ls = np.linspace(-100, 100, 50)\n",
    "assert len(ls) == 50 , 'ls should be of length 50.'\n",
    "assert (min(ls), max(ls)) == (-100, 100) , 'ls should range from -100 to 100, inclusively.'\n",
    "\n",
    "for i, p in enumerate([1/8, 1/4, 1/2, 1, 1.5, 2, 4, 8, 128]):\n",
    "    # TODO: Create a numpy array containing useful values instead of zeros.\n",
    "    # data = np.zeros((2500, 3))\n",
    "    data = np.array([[x, y, pdist((x, y), p)] for x in ls for y in ls])\n",
    "    data[:,2] = data[:,2] / np.max(data[:,2])\n",
    "\n",
    "    assert all(data[:,2] <= 1), 'The third column should be normalized.'\n",
    "\n",
    "    # Plot the data.\n",
    "    colors = [color.to_rgba((0.9, 0.4, 0, 0.7 * (1-a))) for a in data[:,2]]\n",
    "    a = plt.subplot(3, 3, i + 1)\n",
    "    plt.scatter(data[:,0], data[:,1], marker='.', color=colors)\n",
    "    a.set_ylim([-100, 100])\n",
    "    a.set_xlim([-100, 100])\n",
    "    a.set_title('{:.3g}-norm'.format(p))\n",
    "    a.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    figure_norms.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Expectation Maximization [10 Points]\n",
    "\n",
    "In this assignment you will implement the Expectation Maximization algorithm (EM) for 1D data sets.\n",
    "\n",
    "As some parts of this exercise would require some more knowledge of Python than what was already discussed in the practice sessions we built a small number of templates for you to use. However, if you prefer to do so you are also allowed to just go ahead and implement everything yourself! **Don't forget [task b)](#b%29-EM-and-missing-values)**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Implement Expectation Maximization\n",
    "\n",
    "Use the next cell to implement your own solution or, if you want some more guidance, skip the next cell and continue the exercise at  [Step 1) Load the data](#Step-1%29-Load-the-data).\n",
    "\n",
    "Here is an overview of what you have to do:\n",
    "\n",
    "**1) Load the data:**\n",
    "\n",
    "Load the provided data set. It is stored in `em_normdistdata.txt`. We call the set $X$ and each individual data $x \\in X$.\n",
    "\n",
    "**2) Initialize EM:**\n",
    "\n",
    "Initialize three normal distributions whose parameters will be changed iteratively by the EM to converge close to the original distributions.\n",
    "\n",
    "Each normal distribution $j$ has three parameters: $\\mu_j$ (the mean), $\\sigma_j$ (the standard deviation), $\\alpha_j$ (the probability of the normal distribution in the mixture, that means $\\sum\\limits_j\\alpha_j=1$).\n",
    "\n",
    "Initialize the three parameters using three random partitions $S_j$ of the data set. Calculate each $\\mu_j$ and $\\sigma_j$ and set $\\alpha_j = \\frac{|S_j|}{|X|}$.\n",
    "\n",
    "**3) Implement the expectation step:**\n",
    "\n",
    "Perform a soft classification of the data samples with the three normal distributions. That means: Calculate the probability that a data sample $x_i$ belongs to distribution $j$ given parameters $\\mu_j$ and $\\sigma_j$. Or in other words, what is the probability of $x_i$ to be drawn from $N_j(\\mu_j, \\sigma_j)$? When you got the probability, weight the result by $\\alpha_j$.\n",
    "\n",
    "As a last step normalize the results such that the probabilities of a data sample $x_i$ sum up to $1$.\n",
    "\n",
    "**4) Implement the maximization step:**\n",
    "\n",
    "In the maximization step each $\\mu_j$, $\\sigma_j$ and $\\alpha_j$ is updated. First calculate the new means:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij}x_i$$\n",
    "\n",
    "That means $\\mu_j$ is the weighted mean of all samples, where the weight is their probability of belonging to distribution $j$.\n",
    "\n",
    "Then calculate the new $\\sigma_j$. Each new $\\sigma_j$ is the standard deviation of the normal distribution with the new $\\mu_j$, so for the calculation you already use the new $\\mu_j$:\n",
    "\n",
    "$$\\sigma_j = \\sqrt{ \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij} \\left(x_i - \\mu_j\\right)^2 }$$\n",
    "\n",
    "To calculate the new $\\alpha_j$ for each distribution, just take the mean of $p_j$ for each normal distribution $j$.\n",
    "\n",
    "**5) Perform the complete EM and plot your results:**\n",
    "\n",
    "Build a loop around the iterative procedure of expectation and maximization which stops when the changes in all $\\mu_j$ and $\\sigma_j$ are sufficiently small enough.\n",
    "\n",
    "Plot your results after each step and mark which data points belong to which normal distribution. If you don't get it to work, just plot your final solution of the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Free space to implement your own solution -- either use this OR use the following step by step guide. \n",
    "# You may use scipy.stats.norm.pdf for your own implementation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1) Load the data\n",
    "\n",
    "\n",
    "Load the provided data set. It is stored in `em_normdistdata.txt`. We call the set $X$ and each individual data $x \\in X$. \n",
    "\n",
    "*Hint:* Figure out a way on how numpy can load text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Loads the data stored in file_name into a numpy array.\n",
    "    \"\"\"\n",
    "    return np.loadtxt(file_name)\n",
    "\n",
    "assert load_data('em_normdistdata.txt').shape == (200,) , \"The data was not properly loaded.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional:* The data consists of 200 data points drawn from three normal distributions. To get a feeling for the data set you can plot the data with the following cell. Change the number of bins to get a rough idea of how the three distributions might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_data('em_normdistdata.txt')\n",
    "\n",
    "fig_data_test = plt.figure('Data overview')\n",
    "plt.hist(data, bins=5)\n",
    "fig_data_test.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2) Initialize EM\n",
    "\n",
    "Below is a class definition `NormPDF` which represents the probability density function (pdf) of the normal distribution with an additional parameter $\\alpha$. The class is explained in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NormPDF():\n",
    "    \"\"\"\n",
    "    A representation of the probability density function of the normal distribution\n",
    "    for the EM Algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu=0, sigma=1, alpha=1):\n",
    "        \"\"\"\n",
    "        Initializes the normal distribution with mu, sigma and alpha.\n",
    "        The defaults are 0, 1, and 1 respectively.\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Returns the evaluation of this normal distribution at x.\n",
    "        Does not take alpha into account!\n",
    "        \"\"\"\n",
    "        return np.exp(-(x - self.mu) ** 2 / (2 * self.sigma ** 2)) / (np.sqrt(np.pi * 2) * self.sigma)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        A simple string representation of this instance.\n",
    "        \"\"\"\n",
    "        return 'NormPDF({self.mu:.2f},{self.sigma:.2f},{self.alpha:.2f})'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `NormPDF` offers several class methods: `__init__`, `__call__`, `__repr__`. They are all special Python functions which are overloaded so they can be used in a nice way. Note that all methods take as the first parameter `self`: this is just the python way of passing the instance itself to the method so that it becomes possible to access its data. You can always ignore it for now and just assume that the methods only need the parameters which follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "`__init__`: This is the constructor. When a new instance of the class is created this method is used. It takes the parameters `mu`, `sigma`, and `alpha`. Note that if you leave out parameters, they will be set to some default values.\n",
    "So you can create `NormPDF` instances like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = NormPDF()             # No parameters: mu = 0, sigma = 1, alpha = 1\n",
    "b = NormPDF(1)            # mu = 1, sigma = 1, alpha = 1\n",
    "c = NormPDF(1, alpha=0.4) # skips sigma but sets alpha, thus: mu = 1, sigma = 1, alpha = 0.4\n",
    "d = NormPDF(0, 0.5)       # mu = 0, sigma = 0.5, alpha = 1\n",
    "e = NormPDF(0, 0.5, 0.9)  # mu = 0, sigma = 0.5, alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__call__`: This is a very cool feature of Python. By implementing this method one can make an instance *callable*. That basically means one can use it as if it was a function. The `NormPDF` instances can be called with an x value (or a numpy array of x values) to get the evaluation of the normal distribution at x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normpdf = NormPDF()\n",
    "print(normpdf(0))\n",
    "print(normpdf(0.5))\n",
    "print(normpdf(np.linspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__repr__`: This method will be used in Python when one calls `repr(NormPDF())`. As long as `__str__` is not implemented (which you saw in last week's sheet) `str(NormPDF())` will also use this method. This comes in handy for printing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normpdf1 = NormPDF()\n",
    "normpdf2 = NormPDF(1, 0.5, 0.9)\n",
    "print(normpdf1)\n",
    "print([normpdf1, normpdf2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to change the values of an instance of the NormPDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normpdf1 = NormPDF()\n",
    "print(normpdf1)\n",
    "print(normpdf1(np.linspace(-2, 2, 10)))\n",
    "\n",
    "normpdf1.mu = 1\n",
    "normpdf1.sigma = 2\n",
    "normpdf1.alpha = 0.9\n",
    "print(normpdf1)\n",
    "print(normpdf1(np.linspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how the `NormPDF` class works, it is time for the implementation of the initialization function. Here is the task again:\n",
    "\n",
    "Write a function `gaussians = initialize_EM(data, num_distributions)` to initialize the EM. Initialize three normal distributions whose parameters will be changed iteratively by the EM to converge close to the original distributions.\n",
    "\n",
    "Each normal distribution $j$ has three parameters: $\\mu_j$ (the mean), $\\sigma_j$ (the standard deviation), $\\alpha_j$ (the probability of the normal distribution in the mixture, that means $\\sum\\limits_j\\alpha_j=1$).\n",
    "Initialize the three parameters using three random partitions $S_j$ of the data set. Calculate each $\\mu_j$ and $\\sigma_j$ and set $\\alpha_j = \\frac{|S_j|}{|X|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_EM(data, num_distributions):\n",
    "    \"\"\"\n",
    "    Initializes the EM algorithm by calculating num_distributions NormPDFs\n",
    "    from a random partitioning of data.\n",
    "    \"\"\"\n",
    "    partition_mapping = np.random.randint(0, num_distributions, len(data))\n",
    "    gaussians = [NormPDF() for i in range(num_distributions)]\n",
    "    \n",
    "    for index, gaussian in enumerate(gaussians):\n",
    "        gaussians[index].mu = np.mean(data[partition_mapping == index])\n",
    "        gaussians[index].sigma = np.std(data[partition_mapping == index])\n",
    "        gaussians[index].alpha = len(data[partition_mapping == index]) / len(data)\n",
    "    return gaussians\n",
    "\n",
    "\n",
    "normpdfs_ = initialize_EM(np.linspace(-1, 1, 100), 2)\n",
    "assert len(normpdfs_) == 2, \"The number of initialized distributions is not correct.\"\n",
    "assert abs(1 - sum([normpdf.alpha for normpdf in normpdfs_])) < 1e-10 , \"Sum of all alphas is not 1.0!\" # 1e-10 is 0.0000000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3) Implement the expectation step\n",
    "\n",
    "Perform a soft classification of the data samples with the three normal distributions. That means: Calculate the probability that a data sample $x_i$ belongs to distribution $j$ given parameters $\\mu_j$ and $\\sigma_j$. Or in other words, what is the probability of $x_i$ to be drawn from $N_j(\\mu_j, \\sigma_j)$? When you got the probability, weight the result by $\\alpha_j$.\n",
    "\n",
    "As a last step normalize the results such that the probabilities of a data sample $x_i$ sum up to $1$.\n",
    "\n",
    "*Hint:* Store the data in a different array before you normalize it to not run into problems with partly normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expectation_step(gaussians, data):\n",
    "    \"\"\"\n",
    "    Performs the expectation step of the EM.\n",
    "    Returns an array of shape (len(data), len(gaussians))\n",
    "    which contains normalized probabilities for each sample\n",
    "    to denote to which of the normal distributions it \n",
    "    most likely belongs to.\n",
    "    \"\"\"\n",
    "    # Calculates the probabilities of the samples per \n",
    "    # distribution and weights the results by alpha.\n",
    "    tmp = np.empty((len(data), len(gaussians)))\n",
    "    for j, N in enumerate(gaussians):\n",
    "        tmp[:,j] = N.alpha * N(data)\n",
    "\n",
    "    # Normalize the results.\n",
    "    expectation = np.zeros_like(tmp)\n",
    "    for j, N in enumerate(gaussians):\n",
    "        expectation[:,j] = tmp[:,j] / np.sum(tmp[:,:], 1)\n",
    "    return expectation\n",
    "\n",
    "assert expectation_step([NormPDF(), NormPDF()], np.linspace(-2, 2, 100)).shape == (100, 2) , \"Shape is not correct!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4) Implement the maximization step\n",
    "\n",
    "In the maximization step each $\\mu_j$, $\\sigma_j$ and $\\alpha_j$ is updated. First calculate the new means:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij}x_i$$\n",
    "\n",
    "That means $\\mu_j$ is the weighted mean of all samples, where the weight is their probability of belonging to distribution $j$.\n",
    "\n",
    "Then calculate the new $\\sigma_j$. Each new $\\sigma_j$ is the standard deviation of the normal distribution with the new $\\mu_j$, so for the calculation you already use the new $\\mu_j$:\n",
    "\n",
    "$$\\sigma_j = \\sqrt{ \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij} \\left(x_i - \\mu_j\\right)^2 }$$\n",
    "\n",
    "To calculate the new $\\alpha_j$ for each distribution, just take the mean of $p_j$ for each normal distribution $j$.\n",
    "\n",
    "**Caution:** For the next step it is necessary to know how much all $\\mu$ and $\\sigma$ changed. For that the function `maximization_step` should return a numpy array of those (absolute) changes. For example if $\\mu_0$ changed from 0.1 to 0.15, $\\sigma_0$ from 1 to 0.9, $\\mu_1$ from 0.5 to 0.6, and $\\sigma_1$ stayed the same, we expect the function to return `np.array([0.05, 0.1, 0.1, 0])` (however, the order is not important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maximization_step(gaussians, data, expectation):\n",
    "    \"\"\"\n",
    "    Performs the maximization step of the EM.\n",
    "    Modifies the gaussians by updating their mus and sigmas.\n",
    "    Returns a numpy array of absolute changes in any mu or sigma, \n",
    "    that means the returned array has twice as many elements as\n",
    "    the supplied list of gaussians.\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "    for j, N in enumerate(gaussians):\n",
    "        # calculate new parameters\n",
    "        mu = np.sum(expectation[:,j] * data) / np.sum(expectation[:,j])\n",
    "        sigma = np.sqrt(np.sum(expectation[:,j] * (data - mu) ** 2) / np.sum(expectation[:,j]))\n",
    "        alpha = np.mean(expectation[:,j])\n",
    "        \n",
    "        # append relevant changes\n",
    "        changes += [np.abs(N.mu - mu)]\n",
    "        changes += [np.abs(N.sigma - sigma)]\n",
    "        \n",
    "        # update gaussian\n",
    "        N.mu = mu\n",
    "        N.sigma = sigma\n",
    "        N.alpha = alpha\n",
    "    return np.array(changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Perform the complete EM and plot your results:**\n",
    "\n",
    "Build a loop around the iterative procedure of expectation and maximization which stops when the changes in all $\\mu_j$ and $\\sigma_j$ are sufficiently small enough.\n",
    "\n",
    "Plot your results after each step and mark which data points belong to which normal distribution. If you don't get it to work, just plot your final solution.\n",
    "\n",
    "*Hint:* Remember to load the data and initialize the EM before the loop.\n",
    "\n",
    "*Hint:* A function `plot_intermediate_result` to plot your result after each step is already defined in the next cell. Take a look at what arguments it takes and try to use it in your loop.\n",
    "\n",
    "*Hint:* To plot your final result the first three images and corresponding code examples on the tutorial of [`plt.plot(...)`](http://matplotlib.org/users/pyplot_tutorial.html) should help you.\n",
    "\n",
    "*Optional:* Run the code multiple times. If your results are changing, use `np.random.seed(2)` in the beginning of the cell to get consistent results (any other integer will work as well, but 2 has some good results for the example solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2)\n",
    "\n",
    "colors = itertools.cycle(['r', 'g', 'b', 'c', 'm', 'y', 'k'])\n",
    "figure, axis = plt.subplots(1)\n",
    "axis.set_xlim(-5, 5)\n",
    "axis.set_ylim(-0.2, 4)\n",
    "axis.set_title('Intermediate Results')\n",
    "plt.figure('Final Result')\n",
    "def plot_intermediate_result(gaussians, data, mapping):\n",
    "    \"\"\"\n",
    "    Gets a list of gaussians and data input. The mapping\n",
    "    parameter is a list of indices of gaussians. Each value\n",
    "    corresponds to the data value at the same position and \n",
    "    maps this data value to the proper gaussian.\n",
    "    \"\"\"\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    if len(axis.lines):\n",
    "        for j, N in enumerate(gaussians):\n",
    "            axis.lines[j * 2].set_xdata(x)\n",
    "            axis.lines[j * 2].set_ydata(N(x))\n",
    "            axis.lines[j * 2 + 1].set_xdata(data[mapping == j])\n",
    "            axis.lines[j * 2 + 1].set_ydata([0] * len(data[mapping == j]))\n",
    "    else:\n",
    "        for j, N in enumerate(gaussians):\n",
    "            axis.plot(x, N(x), data[mapping == j], [0] * len(data[mapping == j]), 'x', color=next(colors), markersize=5)\n",
    "    figure.canvas.draw()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    \n",
    "# TODO: Perform the initialization.\n",
    "data = load_data('em_normdistdata.txt')\n",
    "gaussians = initialize_EM(data, 3)\n",
    "\n",
    "# TODO: Loop until the changes are small enough.\n",
    "eps = 0.05\n",
    "changes = [float('inf')] * 2\n",
    "while max(changes) > eps:\n",
    "    # TODO: Iteratively apply the expectation step, followed by the maximization step.\n",
    "    expectation = expectation_step(gaussians, data)\n",
    "    changes = maximization_step(gaussians, data, expectation)\n",
    "    \n",
    "    # (Optional:) TODO: Calculate the parameters to update the plot and call the function to do it.\n",
    "    plot_intermediate_result(gaussians, data, np.argmax(expectation[:], 1))\n",
    "    \n",
    "# TODO: Plot your final result and print the final parameters.\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "plt.plot(x, gaussians[0](x), 'r', x, gaussians[1](x), 'g', x, gaussians[2](x), 'b')\n",
    "print(gaussians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### b) EM and missing values\n",
    "\n",
    "Describe in your own words: How does the EM-algorithm deal with the missing value problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the EM-Algorithm all known values are considered via their probability depending on the distribution. In the same way hidden (i.e. missing) values are considered as depending on the probability distribution and additionally on the known values. So the complete distribution can be seen as the product of two probability distributions (known and missing values).\n",
    "\n",
    "The algorithm searches for the parameters that maximize the log-likelihood. As they depend on the missing values, those are averaged out. In an iterative procedure the estimated parameter is improved (M-step) followed by averaging over the missing values using the obtained parameter (E-step). This will lead the estimation of the parameter to converge to a local maximum which hopefully is close to the real parameter value. The principle in handling missing values here is to not try to regain them somehow, but to invent values from a model optained through the probability distribution. In the best case this does not lead to information loss, although it generally does. However, this at least makes the existing values technically usable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
