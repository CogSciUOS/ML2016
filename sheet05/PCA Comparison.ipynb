{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates different methods of calculating the PCA and how they behave with different data.\n",
    "\n",
    "For the small arbitrary default matrix of size $3 \\times 2$ we did the calculations (raw data) by hand to compare them to the results, you can find them below. Additionally you can choose an arbitrary $4 \\times 3$ matrix or the `cars.csv` from our homework conveniently.\n",
    "\n",
    "The tests are run three times: Once for the raw data, once for the centered data and once for the normalized data (i.e. mean 0 and std 1).\n",
    "\n",
    "In the plots and text outputs (set `TEXTOUT = True`!) created when you run the cell, the following names are used:\n",
    "\n",
    "* `eig`: Calculates the covariance matrix and uses [`numpy.linalg.eig`](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig) (mostly equivalent to [`scipy.linalg.eig`](http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.linalg.eig.html)) to calculate the eigenvalues and -vectors.\n",
    "* `eigh`: Calculates the covariance matrix and uses [`numpy.linalg.eigh`](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linalg.eigh.html#numpy.linalg.eigh) to calculate the eigenvalues and -vectors.\n",
    "* `svd`: Uses [`numpy.linalg.svd`](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linalg.svd.html#numpy.linalg.svd) to calculate the eigenvalues and -vectors.\n",
    "* `PCA`: Application of [`sklearn.decomposition.PCA`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Internally centers the data and uses svd.\n",
    "* `IncrementalPCA`: Application of [`sklearn.decomposition.IncrementalPCA`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html). Internally centers the data and uses svd.\n",
    "* `Eigenvalues`: Plots a comparison of all resulting eigenvalues.\n",
    "\n",
    "Note that there are even more interesting methods to calculate principal components or for example independent components. An overview of algorithms can be found in the [scikit-learn user guide on decomposition](http://scikit-learn.org/stable/modules/decomposition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enables text output.\n",
    "TEXTOUT = False \n",
    "\n",
    "# Choose a data set.\n",
    "# data = np.array([[3,2,-1],[4,1,2],[4,2,9],[2,-3,1]])\n",
    "data = np.array([[5, 2], [3, 4], [6, 1]])\n",
    "# data = np.loadtxt('cars.csv', delimiter=',')\n",
    "\n",
    "\n",
    "# Calculates normalization.\n",
    "mean = np.mean(data, 0)\n",
    "std = np.std(data, 0)\n",
    "\n",
    "eigval = OrderedDict({'Raw Data': {}, 'Centered Data': {}, 'Normalized Data': {}})\n",
    "eigvec = OrderedDict({'Raw Data': {}, 'Centered Data': {}, 'Normalized Data': {}})\n",
    "\n",
    "data = OrderedDict({'Raw Data': data, 'Centered Data': data - mean, 'Normalized Data': (data - mean) / std})\n",
    "\n",
    "def pca_eig(data):\n",
    "    \"\"\"Uses numpy.linalg.eig to calculate the PCA.\"\"\"\n",
    "    data = data.T @ data\n",
    "    val, vec = np.linalg.eig(data)\n",
    "    sort = np.argsort(val)\n",
    "    return val[sort], vec[:,sort]\n",
    "\n",
    "def pca_eigh(data):\n",
    "    \"\"\"Uses numpy.linalg.eigh to calculate the PCA.\"\"\"\n",
    "    data = data.T @ data\n",
    "    val, vec = np.linalg.eigh(data)\n",
    "    sort = np.argsort(val)\n",
    "    return val[sort], vec[:,sort]\n",
    "\n",
    "def pca_svd(data):\n",
    "    \"\"\"Uses numpy.linalg.svd to calculate the PCA.\"\"\"\n",
    "    u, s, v = np.linalg.svd(data)\n",
    "    val, vec = s ** 2, v\n",
    "    sort = np.argsort(val)\n",
    "    return val[sort], vec[:,sort]\n",
    "    \n",
    "def pca_PCA(data):\n",
    "    \"\"\"Uses sklearn.decomposition.PCA to calculate the PCA.\"\"\"\n",
    "    pca = sklearn.decomposition.PCA().fit(data)\n",
    "    val, vec = pca.explained_variance_, pca.components_\n",
    "    sort = np.argsort(val)\n",
    "    return val[sort], vec[:,sort]\n",
    "\n",
    "def pca_IncrPCA(data):\n",
    "    \"\"\"Uses sklearn.decomposition.IncrementalPCA to calculate the PCA.\"\"\"\n",
    "    pca = sklearn.decomposition.IncrementalPCA().fit(data)\n",
    "    val, vec = pca.explained_variance_, pca.components_\n",
    "    sort = np.argsort(val)\n",
    "    return val[sort], vec[:,sort]\n",
    "\n",
    "pcas = {\n",
    "    'eig': pca_eig,\n",
    "    'eigh': pca_eigh,\n",
    "    'svd': pca_svd,\n",
    "    'PCA': pca_PCA,\n",
    "    'IncrementalPCA': pca_IncrPCA,\n",
    "}\n",
    "\n",
    "for k, v in data.items():\n",
    "    figure = plt.figure(k, figsize=(10,8))\n",
    "    i = 1\n",
    "    for name, pca in pcas.items():\n",
    "        eigvalues, eigvecs = pca(v)\n",
    "        eigval[k][name] = eigvalues\n",
    "        eigvec[k][name] = eigvecs\n",
    "        transformed = v @ eigvecs[:,0:2]\n",
    "    \n",
    "        figure = plt.figure(k)\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.gca().set_title(name)\n",
    "        plt.scatter(*zip(*transformed))\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.plot(sorted(eigval[k][name]), linestyle='None', marker='o+xd.'[i-1], label=name)\n",
    "        plt.legend(loc=2, prop={'size':6})\n",
    "        i += 1\n",
    "    plt.gca().set_title('Eigenvalues')\n",
    "\n",
    "if TEXTOUT:\n",
    "    for k, v in data.items():\n",
    "        print(k)\n",
    "        for name in pcas.keys():\n",
    "            print('\\t', name)\n",
    "            print('\\t\\tEigenvalues:')\n",
    "            print('\\t\\t', eigval[k][name])\n",
    "            print('\\t\\tEigenvectors:')\n",
    "            print('\\t\\t', eigvec[k][name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the PCA by hand for the matrix $A = \\left(\\matrix{ 5 & 2 \\\\ 3 & 4 \\\\ 6 & 1 }\\right)$, which is also one of the examples above.\n",
    "\n",
    "First calculate our covariance matrix:\n",
    "\n",
    "$$\\begin{align*}\n",
    "       C \n",
    "    &= A^TA \\\\\n",
    "    &= \\left(\\matrix{ 5 & 3 & 6 \\\\ 2 & 4 & 1 }\\right) \\left(\\matrix{ 5 & 2 \\\\ 3 & 4 \\\\ 6 & 1 }\\right) \\\\\n",
    "    &= \\left(\\matrix{ 70 & 28 \\\\ 28 & 21 }\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "The characteristic polynomial of $C$ is ($I$ is the identity matrix):\n",
    "\n",
    "$$\\begin{align*}\n",
    "       \\left| C - \\lambda I \\right|\n",
    "    &= \\left| \\left(\\matrix{ 70 & 28 \\\\ 28 & 21 }\\right) - \\lambda \\left(\\matrix{ 1 & 0 \\\\ 0 & 1 }\\right) \\right| \\\\\n",
    "    &= \\left| \\left(\\matrix{ 70 - \\lambda & 28 \\\\ 28 & 21 - \\lambda }\\right) \\right|\\\\\n",
    "    &\\Rightarrow \\lambda^2 - \\lambda\\, {\\rm tr}(C) + \\det(C)\\\\\n",
    "    &=\\lambda^2 - (70 + 21)\\lambda + (70 \\cdot 21 - 28^2)\\\\\n",
    "    &=\\lambda^2 - 91\\lambda + 686\n",
    "\\end{align*}$$\n",
    "\n",
    "We need to find the roots:\n",
    "\n",
    "$$\\begin{align*}\n",
    "       \\lambda \n",
    "    &= -\\frac{91}{2} \\pm \\sqrt{ \\left(\\frac{91}{2}\\right)^2 - 686 } \\\\\n",
    "    &= -45.5 \\pm \\sqrt{ 1384.25 } \\\\\n",
    "    &\\Rightarrow \\lambda_1 \\approx 82.71\\\\\n",
    "    &\\Rightarrow \\lambda_2 \\approx 8.29\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "With the two eigenvalues we can now calculate the eigenvectors by solving the characteristic polynomial:\n",
    "\n",
    "$$\\begin{align*}\n",
    "        \\lambda_1: E_1 \n",
    "    &= \\left| \\left(\\matrix{ 70 & 28 \\\\ 28 & 21 }\\right) - \\lambda_1 \\left(\\matrix{ 1 & 0 \\\\ 0 & 1 }\\right) \\right| \\\\\n",
    "    &= \\left(\\matrix{ -12.71 & 28 \\\\ 28 & -61.71 }\\right)\n",
    "\\end{align*}$$\n",
    "$$\\begin{align*}\n",
    "        \\lambda_2: E_2 \n",
    "    &= \\left| \\left(\\matrix{ 70 & 28 \\\\ 28 & 21 }\\right) - \\lambda_2 \\left(\\matrix{ 1 & 0 \\\\ 0 & 1 }\\right) \\right| \\\\\n",
    "    &= \\left(\\matrix{ 61.71 & 28 \\\\ 28 & 12.71 }\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Taking one column vector of each of the results results in possible eigenvectors. So we have:\n",
    "\n",
    "|Eigenvalue | possible Eigenvectors                                                         |\n",
    "|-----------|-------------------------------------------------------------------------------|\n",
    "|   $82.71$ | $\\left(\\matrix{ -12.71 \\\\ 28 }\\right)$ $\\left(\\matrix{ 28 \\\\ -61.71 }\\right)$ |\n",
    "|    $8.29$ | $\\left(\\matrix{  61.71 \\\\ 28 }\\right)$ $\\left(\\matrix{ 28 \\\\  12.71 }\\right)$ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
